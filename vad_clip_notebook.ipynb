{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aNpPTDBgNr1nK63ztQXatuHhXMJLsGAa","authorship_tag":"ABX9TyNTLJiulwWMiAyKZWem9/Wh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"XxKtd3_hImCC","executionInfo":{"status":"ok","timestamp":1751795316859,"user_tz":-300,"elapsed":93,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"cd84e735-d98c-451b-9c5e-737d94787ce0"},"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}],"source":["#@title Mount Drive\n","import os\n","if os.path.ismount('/content/drive'):\n","    print(\"Google Drive is already mounted.\")\n","else:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DfMHfDVZPB8","executionInfo":{"status":"ok","timestamp":1751802127095,"user_tz":-300,"elapsed":11901,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"95b3f483-0474-4995-bc9d-cee490569c51"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n"]}]},{"cell_type":"code","source":["import os\n","\n","from pathlib import Path"],"metadata":{"id":"F_pOyYg7Ogdd","executionInfo":{"status":"ok","timestamp":1751802128883,"user_tz":-300,"elapsed":37,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#@title Pull Latest Repo\n","NOTEBOOK_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/vad_clip\")\n","NOTEBOOK_PATH = NOTEBOOK_DIR / \"vad_clip_notebook.ipynb\"\n","\n","with open(NOTEBOOK_DIR / \"vad_clip_gh_token.txt\") as f:\n","  gh_token = f.read().strip()\n","\n","REPO_NAME = \"vad_clip_colab_notebook\"\n","REPO_PATH = Path(\"/content\") / REPO_NAME\n","GITHUB_URL = f\"https://{gh_token}@github.com/hammad-ali1/{REPO_NAME}.git\"\n","\n","!rm -rf {REPO_PATH}\n","!git clone {GITHUB_URL}\n","\n","!cp -r {REPO_PATH}/* /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3JvbCrKXHfc","executionInfo":{"status":"ok","timestamp":1751802227477,"user_tz":-300,"elapsed":1358,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"ad980694-6f03-4a26-ec4c-7a7f84ce095e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vad_clip_colab_notebook'...\n","remote: Enumerating objects: 52, done.\u001b[K\n","remote: Counting objects: 100% (52/52), done.\u001b[K\n","remote: Compressing objects: 100% (43/43), done.\u001b[K\n","remote: Total 52 (delta 8), reused 45 (delta 7), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (52/52), 1.62 MiB | 3.63 MiB/s, done.\n","Resolving deltas: 100% (8/8), done.\n"]}]},{"cell_type":"code","source":["#@title Push Notebook to Github\n","commit_msg = input(\"Commit message: \")\n","!cp \"{NOTEBOOK_PATH}\" {REPO_PATH}\n","!cd {REPO_PATH} && git config user.name \"Hammad Ali\"\n","!cd {REPO_PATH} && git config user.email \"hammad.a22002@gmail.com\"\n","\n","# Add, commit and push\n","!cd {REPO_PATH} && git add .\n","!cd {REPO_PATH} && git commit -m {commit_msg}\n","!cd {REPO_PATH} && git push"],"metadata":{"id":"CAuhT-UbJJ6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Args:\n","    seed = 234\n","\n","    embed_dim = 512\n","    visual_length = 256\n","    visual_width = 512\n","    visual_head = 1\n","    visual_layers = 2\n","    attn_window = 8\n","    prompt_prefix = 10\n","    prompt_postfix = 10\n","    classes_num = 14\n","\n","    max_epoch = 10\n","    model_path = NOTEBOOK_DIR / \"model\" / \"model_cur.pth\"\n","    use_checkpoint = False\n","    checkpoint_path = NOTEBOOK_DIR / \"model\" / \"checkpoint_v2.pth\"\n","    batch_size = 64\n","    train_list = 'list/ucf_CLIP_rgb.csv'\n","    test_list = 'list/ucf_CLIP_rgbtest.csv'\n","    gt_path = 'list/gt_ucf.npy'\n","    gt_segment_path = 'list/gt_segment_ucf.npy'\n","    gt_label_path = 'list/gt_label_ucf.npy'\n","\n","    lr = 2e-5\n","    scheduler_rate = 0.1\n","    scheduler_milestones = [4, 8]\n"],"metadata":{"id":"0VPhYpQPbBSN","executionInfo":{"status":"ok","timestamp":1751802248570,"user_tz":-300,"elapsed":45,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title Model\n","from collections import OrderedDict\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from clip import clip\n","from utils.layers import GraphConvolution, DistanceAdj\n","\n","class LayerNorm(nn.LayerNorm):\n","\n","    def forward(self, x: torch.Tensor):\n","        orig_type = x.dtype\n","        ret = super().forward(x.type(torch.float32))\n","        return ret.type(orig_type)\n","\n","\n","class QuickGELU(nn.Module):\n","    def forward(self, x: torch.Tensor):\n","        return x * torch.sigmoid(1.702 * x)\n","\n","\n","class ResidualAttentionBlock(nn.Module):\n","    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n","        super().__init__()\n","\n","        self.attn = nn.MultiheadAttention(d_model, n_head)\n","        self.ln_1 = LayerNorm(d_model)\n","        self.mlp = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n","        ]))\n","        self.ln_2 = LayerNorm(d_model)\n","        self.attn_mask = attn_mask\n","\n","    def attention(self, x: torch.Tensor, padding_mask: torch.Tensor):\n","        padding_mask = padding_mask.to(dtype=bool, device=x.device) if padding_mask is not None else None\n","        self.attn_mask = self.attn_mask.to(device=x.device) if self.attn_mask is not None else None\n","        return self.attn(x, x, x, need_weights=False, key_padding_mask=padding_mask, attn_mask=self.attn_mask)[0]\n","\n","    def forward(self, x):\n","        x, padding_mask = x\n","        x = x + self.attention(self.ln_1(x), padding_mask)\n","        x = x + self.mlp(self.ln_2(x))\n","        return (x, padding_mask)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n","        super().__init__()\n","        self.width = width\n","        self.layers = layers\n","        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n","\n","    def forward(self, x: torch.Tensor):\n","        return self.resblocks(x)\n","\n","\n","class CLIPVAD(nn.Module):\n","    def __init__(self,\n","                 num_class: int,\n","                 embed_dim: int,\n","                 visual_length: int,\n","                 visual_width: int,\n","                 visual_head: int,\n","                 visual_layers: int,\n","                 attn_window: int,\n","                 prompt_prefix: int,\n","                 prompt_postfix: int,\n","                 device):\n","        super().__init__()\n","\n","        self.num_class = num_class\n","        self.visual_length = visual_length\n","        self.visual_width = visual_width\n","        self.embed_dim = embed_dim\n","        self.attn_window = attn_window\n","        self.prompt_prefix = prompt_prefix\n","        self.prompt_postfix = prompt_postfix\n","        self.device = device\n","\n","        self.temporal = Transformer(\n","            width=visual_width,\n","            layers=visual_layers,\n","            heads=visual_head,\n","            attn_mask=self.build_attention_mask(self.attn_window)\n","        )\n","\n","        width = int(visual_width / 2)\n","        self.gc1 = GraphConvolution(visual_width, width, residual=True)\n","        self.gc2 = GraphConvolution(width, width, residual=True)\n","        self.gc3 = GraphConvolution(visual_width, width, residual=True)\n","        self.gc4 = GraphConvolution(width, width, residual=True)\n","        self.disAdj = DistanceAdj()\n","        self.linear = nn.Linear(visual_width, visual_width)\n","        self.gelu = QuickGELU()\n","\n","        self.mlp1 = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n","        ]))\n","        self.mlp2 = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n","        ]))\n","        self.classifier = nn.Linear(visual_width, 1)\n","\n","        self.clipmodel, _ = clip.load(\"ViT-B/16\", device)\n","        for clip_param in self.clipmodel.parameters():\n","            clip_param.requires_grad = False\n","\n","        self.frame_position_embeddings = nn.Embedding(visual_length, visual_width)\n","        self.text_prompt_embeddings = nn.Embedding(77, self.embed_dim)\n","\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        nn.init.normal_(self.text_prompt_embeddings.weight, std=0.01)\n","        nn.init.normal_(self.frame_position_embeddings.weight, std=0.01)\n","\n","    def build_attention_mask(self, attn_window):\n","        # lazily create causal attention mask, with full attention between the vision tokens\n","        # pytorch uses additive attention mask; fill with -inf\n","        mask = torch.empty(self.visual_length, self.visual_length)\n","        mask.fill_(float('-inf'))\n","        for i in range(int(self.visual_length / attn_window)):\n","            if (i + 1) * attn_window < self.visual_length:\n","                mask[i * attn_window: (i + 1) * attn_window, i * attn_window: (i + 1) * attn_window] = 0\n","            else:\n","                mask[i * attn_window: self.visual_length, i * attn_window: self.visual_length] = 0\n","\n","        return mask\n","\n","    def adj4(self, x, seq_len):\n","        soft = nn.Softmax(1)\n","        x2 = x.matmul(x.permute(0, 2, 1)) # B*T*T\n","        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n","        x_norm_x = x_norm.matmul(x_norm.permute(0, 2, 1))\n","        x2 = x2/(x_norm_x+1e-20)\n","        output = torch.zeros_like(x2)\n","        if seq_len is None:\n","            for i in range(x.shape[0]):\n","                tmp = x2[i]\n","                adj2 = tmp\n","                adj2 = F.threshold(adj2, 0.7, 0)\n","                adj2 = soft(adj2)\n","                output[i] = adj2\n","        else:\n","            for i in range(len(seq_len)):\n","                tmp = x2[i, :seq_len[i], :seq_len[i]]\n","                adj2 = tmp\n","                adj2 = F.threshold(adj2, 0.7, 0)\n","                adj2 = soft(adj2)\n","                output[i, :seq_len[i], :seq_len[i]] = adj2\n","\n","        return output\n","\n","    def encode_video(self, images, padding_mask, lengths):\n","        images = images.to(torch.float)\n","        position_ids = torch.arange(self.visual_length, device=self.device)\n","        position_ids = position_ids.unsqueeze(0).expand(images.shape[0], -1)\n","        frame_position_embeddings = self.frame_position_embeddings(position_ids)\n","        frame_position_embeddings = frame_position_embeddings.permute(1, 0, 2)\n","        images = images.permute(1, 0, 2) + frame_position_embeddings\n","\n","        x, _ = self.temporal((images, None))\n","        x = x.permute(1, 0, 2)\n","\n","        adj = self.adj4(x, lengths)\n","        disadj = self.disAdj(x.shape[0], x.shape[1])\n","        x1_h = self.gelu(self.gc1(x, adj))\n","        x2_h = self.gelu(self.gc3(x, disadj))\n","\n","        x1 = self.gelu(self.gc2(x1_h, adj))\n","        x2 = self.gelu(self.gc4(x2_h, disadj))\n","\n","        x = torch.cat((x1, x2), 2)\n","        x = self.linear(x)\n","\n","        return x\n","\n","    def encode_textprompt(self, text):\n","        word_tokens = clip.tokenize(text).to(self.device)\n","        word_embedding = self.clipmodel.encode_token(word_tokens)\n","        text_embeddings = self.text_prompt_embeddings(torch.arange(77).to(self.device)).unsqueeze(0).repeat([len(text), 1, 1])\n","        text_tokens = torch.zeros(len(text), 77).to(self.device)\n","\n","        for i in range(len(text)):\n","            ind = torch.argmax(word_tokens[i], -1)\n","            text_embeddings[i, 0] = word_embedding[i, 0]\n","            text_embeddings[i, self.prompt_prefix + 1: self.prompt_prefix + ind] = word_embedding[i, 1: ind]\n","            text_embeddings[i, self.prompt_prefix + ind + self.prompt_postfix] = word_embedding[i, ind]\n","            text_tokens[i, self.prompt_prefix + ind + self.prompt_postfix] = word_tokens[i, ind]\n","\n","        text_features = self.clipmodel.encode_text(text_embeddings, text_tokens)\n","\n","        return text_features\n","\n","    def forward(self, visual, padding_mask, text, lengths):\n","        visual_features = self.encode_video(visual, padding_mask, lengths)\n","        logits1 = self.classifier(visual_features + self.mlp2(visual_features))\n","\n","        text_features_ori = self.encode_textprompt(text)\n","\n","        text_features = text_features_ori\n","        logits_attn = logits1.permute(0, 2, 1)\n","        visual_attn = logits_attn @ visual_features\n","        visual_attn = visual_attn / visual_attn.norm(dim=-1, keepdim=True)\n","        visual_attn = visual_attn.expand(visual_attn.shape[0], text_features_ori.shape[0], visual_attn.shape[2])\n","        text_features = text_features_ori.unsqueeze(0)\n","        text_features = text_features.expand(visual_attn.shape[0], text_features.shape[1], text_features.shape[2])\n","        text_features = text_features + visual_attn\n","        text_features = text_features + self.mlp1(text_features)\n","\n","        visual_features_norm = visual_features / visual_features.norm(dim=-1, keepdim=True)\n","        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n","        text_features_norm = text_features_norm.permute(0, 2, 1)\n","        logits2 = visual_features_norm @ text_features_norm.type(visual_features_norm.dtype) / 0.07\n","\n","        return text_features_ori, logits1, logits2\n",""],"metadata":{"id":"2ODZ8AzzN3_M","executionInfo":{"status":"ok","timestamp":1751802261640,"user_tz":-300,"elapsed":8839,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#@title Test Function\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from sklearn.metrics import average_precision_score, roc_auc_score\n","\n","from utils.dataset import UCFDataset\n","from utils.tools import get_batch_mask, get_prompt_text\n","from utils.ucf_detectionMAP import getDetectionMAP as dmAP\n","\n","def test(model, testdataloader, maxlen, prompt_text, gt, gtsegments, gtlabels, device):\n","\n","    model.to(device)\n","    model.eval()\n","\n","    element_logits2_stack = []\n","\n","    with torch.no_grad():\n","        for i, item in enumerate(testdataloader):\n","            visual = item[0].squeeze(0)\n","            length = item[2]\n","\n","            length = int(length)\n","            len_cur = length\n","            if len_cur < maxlen:\n","                visual = visual.unsqueeze(0)\n","\n","            visual = visual.to(device)\n","\n","            lengths = torch.zeros(int(length / maxlen) + 1)\n","            for j in range(int(length / maxlen) + 1):\n","                if j == 0 and length < maxlen:\n","                    lengths[j] = length\n","                elif j == 0 and length > maxlen:\n","                    lengths[j] = maxlen\n","                    length -= maxlen\n","                elif length > maxlen:\n","                    lengths[j] = maxlen\n","                    length -= maxlen\n","                else:\n","                    lengths[j] = length\n","            lengths = lengths.to(int)\n","            padding_mask = get_batch_mask(lengths, maxlen).to(device)\n","            _, logits1, logits2 = model(visual, padding_mask, prompt_text, lengths)\n","            logits1 = logits1.reshape(logits1.shape[0] * logits1.shape[1], logits1.shape[2])\n","            logits2 = logits2.reshape(logits2.shape[0] * logits2.shape[1], logits2.shape[2])\n","            prob2 = (1 - logits2[0:len_cur].softmax(dim=-1)[:, 0].squeeze(-1))\n","            prob1 = torch.sigmoid(logits1[0:len_cur].squeeze(-1))\n","\n","            if i == 0:\n","                ap1 = prob1\n","                ap2 = prob2\n","                #ap3 = prob3\n","            else:\n","                ap1 = torch.cat([ap1, prob1], dim=0)\n","                ap2 = torch.cat([ap2, prob2], dim=0)\n","\n","            element_logits2 = logits2[0:len_cur].softmax(dim=-1).detach().cpu().numpy()\n","            element_logits2 = np.repeat(element_logits2, 16, 0)\n","            element_logits2_stack.append(element_logits2)\n","\n","    ap1 = ap1.cpu().numpy()\n","    ap2 = ap2.cpu().numpy()\n","    ap1 = ap1.tolist()\n","    ap2 = ap2.tolist()\n","\n","    ROC1 = roc_auc_score(gt, np.repeat(ap1, 16))\n","    AP1 = average_precision_score(gt, np.repeat(ap1, 16))\n","    ROC2 = roc_auc_score(gt, np.repeat(ap2, 16))\n","    AP2 = average_precision_score(gt, np.repeat(ap2, 16))\n","\n","    print(\"AUC1: \", ROC1, \" AP1: \", AP1)\n","    print(\"AUC2: \", ROC2, \" AP2:\", AP2)\n","\n","    dmap, iou = dmAP(element_logits2_stack, gtsegments, gtlabels, excludeNormal=False)\n","    averageMAP = 0\n","    for i in range(5):\n","        print('mAP@{0:.1f} ={1:.2f}%'.format(iou[i], dmap[i]))\n","        averageMAP += dmap[i]\n","    averageMAP = averageMAP/(i+1)\n","    print('average MAP: {:.2f}'.format(averageMAP))\n","\n","    return ROC1, AP1\n","\n","\n","def run_test():\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args = Args()\n","\n","    label_map = dict({'Normal': 'Normal', 'Abuse': 'Abuse', 'Arrest': 'Arrest', 'Arson': 'Arson', 'Assault': 'Assault', 'Burglary': 'Burglary', 'Explosion': 'Explosion', 'Fighting': 'Fighting', 'RoadAccidents': 'RoadAccidents', 'Robbery': 'Robbery', 'Shooting': 'Shooting', 'Shoplifting': 'Shoplifting', 'Stealing': 'Stealing', 'Vandalism': 'Vandalism'})\n","\n","    testdataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n","    testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)\n","\n","    prompt_text = get_prompt_text(label_map)\n","    gt = np.load(args.gt_path)\n","    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n","    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n","\n","    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n","    model_param = torch.load(args.model_path)\n","    model.load_state_dict(model_param)\n","\n","    test(model, testdataloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)"],"metadata":{"id":"8LrMWNfSZ-RX","executionInfo":{"status":"ok","timestamp":1751802267469,"user_tz":-300,"elapsed":1125,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["run_test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVqGqcItfRlQ","executionInfo":{"status":"ok","timestamp":1751802512643,"user_tz":-300,"elapsed":242401,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"0b089dc5-55d0-417f-d8d7-72b7b385ccb0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 335M/335M [00:07<00:00, 49.4MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["AUC1:  0.8797552244114377  AP1:  0.34538302375548924\n","AUC2:  0.8648480514983509  AP2: 0.28767348997404496\n","mAP@0.1 =13.25%\n","mAP@0.2 =9.18%\n","mAP@0.3 =6.77%\n","mAP@0.4 =4.41%\n","mAP@0.5 =3.73%\n","average MAP: 7.47\n"]}]}]}