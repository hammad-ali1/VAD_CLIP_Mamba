{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hammad-ali1/vad_clip_colab_notebook/blob/main/vad_clip_notebook.ipynb)\n"
      ],
      "metadata": {
        "id": "hjJEkZzImzOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install ftfy\n",
        "!pip install comet_ml\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "6DfMHfDVZPB8",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pull Latest Repo\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "  gh_token = userdata.get('vad_clip_gh_token')\n",
        "except Exception:\n",
        "  gh_token = None\n",
        "\n",
        "REPO_NAME = \"vad_clip_colab_notebook\"\n",
        "REPO_PATH = Path(\"/content\") / REPO_NAME\n",
        "\n",
        "if gh_token:\n",
        "  GITHUB_URL = f\"https://{gh_token}@github.com/hammad-ali1/{REPO_NAME}.git\"\n",
        "else:\n",
        "  GITHUB_URL = f'https://github.com/hammad-ali1/{REPO_NAME}'\n",
        "\n",
        "!rm -rf {REPO_PATH}\n",
        "!git clone {GITHUB_URL}\n",
        "\n",
        "!cp -r {REPO_PATH}/* /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3JvbCrKXHfc",
        "outputId": "645657d1-a068-40a8-bbdf-dec16670cafc",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vad_clip_colab_notebook'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 88 (delta 30), reused 52 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (88/88), 1.68 MiB | 3.81 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Modules\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "import comet_ml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "from clip import clip\n",
        "from utils.dataset import UCFDataset\n",
        "from utils.layers import GraphConvolution, DistanceAdj\n",
        "from utils.tools import get_batch_mask, get_prompt_text, get_batch_label\n",
        "from utils.ucf_detectionMAP import getDetectionMAP as dmAP"
      ],
      "metadata": {
        "id": "F_pOyYg7Ogdd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_DATASET_DIR = Path('/content/drive/MyDrive/UCFClipFeatures')\n",
        "\n",
        "if os.path.ismount('/content/drive') and DRIVE_DATASET_DIR.exists():\n",
        "  DATASET_DIR = DRIVE_DATASET_DIR\n",
        "else:\n",
        "  !gdown 1DOkXHObwEGqmU3c9-J1gCIyVXiTMFfC5\n",
        "  !unzip /content/UCFClipFeatures.zip -d /content\n",
        "  !rm /content/UCFClipFeatures.zip\n",
        "  DATASET_DIR = Path('/content/UCFClipFeatures')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "aXPCB6SdOqzk",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    seed = 234\n",
        "\n",
        "    embed_dim = 512\n",
        "    visual_length = 256\n",
        "    visual_width = 512\n",
        "    visual_head = 1\n",
        "    visual_layers = 2\n",
        "    attn_window = 8\n",
        "    prompt_prefix = 10\n",
        "    prompt_postfix = 10\n",
        "    classes_num = 14\n",
        "\n",
        "    max_epoch = 10\n",
        "    model_path = \"model_cur.pth\"\n",
        "    use_checkpoint = True\n",
        "    checkpoint_experiment_key = 'ec92e781207d4201a0961ddf8f78eb30'\n",
        "    checkpoint_file = \"best_checkpoint.pth\"\n",
        "    batch_size = 64\n",
        "    train_list = 'list/ucf_CLIP_rgb.csv'\n",
        "    test_list = 'list/ucf_CLIP_rgbtest.csv'\n",
        "    gt_path = 'list/gt_ucf.npy'\n",
        "    gt_segment_path = 'list/gt_segment_ucf.npy'\n",
        "    gt_label_path = 'list/gt_label_ucf.npy'\n",
        "\n",
        "    lr = 2e-5\n",
        "    scheduler_rate = 0.1\n",
        "    scheduler_milestones = [4, 8]\n"
      ],
      "metadata": {
        "id": "0VPhYpQPbBSN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMET_API_KEY = userdata.get('COMET_API_KEY')\n",
        "COMET_PROJECT_NAME = \"vad_clip_notebook\"\n",
        "COMET_WORKSPACE = 'hammad-ali'"
      ],
      "metadata": {
        "id": "Lv0xzsL-WtCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_commet_asset(file_name):\n",
        "  comet_api = comet_ml.API(api_key=COMET_API_KEY)\n",
        "  checkpoint_experiment = comet_api.get_experiment(COMET_WORKSPACE, COMET_PROJECT_NAME, Args.checkpoint_experiment_key)\n",
        "  asset_link = [asset for asset in checkpoint_experiment.get_asset_list()\n",
        "                   if asset['fileName'] == file_name][0]['s3Link']\n",
        "  !curl -o {file_name} \"{asset_link}\""
      ],
      "metadata": {
        "id": "8Ui0g7gMXCSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_lists(file_path, limit_list=None):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract video ID (removing part suffix)\n",
        "    df['video_id'] = df['path'].apply(lambda x: x.rsplit('__', 1)[0])\n",
        "\n",
        "    # Keep only complete videos (with all 10 parts)\n",
        "    grouped = df.groupby('video_id')\n",
        "    complete_videos = grouped.filter(lambda x: len(x) == 10)\n",
        "\n",
        "    if limit_list is not None:\n",
        "        total_videos = limit_list // 10\n",
        "\n",
        "        # Create video-level metadata\n",
        "        video_meta = (\n",
        "            complete_videos.groupby('video_id')\n",
        "            .first()\n",
        "            .reset_index()[['video_id', 'label']]\n",
        "        )\n",
        "\n",
        "        # Separate Normal and Other labels\n",
        "        normal_videos = video_meta[video_meta['label'] == 'Normal']\n",
        "        other_videos = video_meta[video_meta['label'] != 'Normal']\n",
        "        other_labels = other_videos['label'].unique()\n",
        "\n",
        "        num_normal_videos = total_videos // 2\n",
        "        num_other_videos = total_videos - num_normal_videos\n",
        "        per_other_label = max(1, num_other_videos // len(other_labels))\n",
        "\n",
        "        # Sample Normal videos\n",
        "        selected_videos = normal_videos.sample(\n",
        "            n=min(num_normal_videos, len(normal_videos)), random_state=42\n",
        "        )\n",
        "\n",
        "        # Sample from other labels\n",
        "        for label in other_labels:\n",
        "            vids = other_videos[other_videos['label'] == label]\n",
        "            sampled = vids.sample(n=min(per_other_label, len(vids)), random_state=42)\n",
        "            selected_videos = pd.concat([selected_videos, sampled], ignore_index=True)\n",
        "\n",
        "        # Get all 10 parts of selected videos\n",
        "        final_df = complete_videos[complete_videos['video_id'].isin(selected_videos['video_id'])]\n",
        "\n",
        "        # Sort by video and clip number\n",
        "        final_df['part'] = final_df['path'].apply(lambda x: int(Path(x).stem.split('__')[-1]))\n",
        "        final_df = final_df.sort_values(by=['label', 'video_id', 'part']).drop(columns='part')\n",
        "    else:\n",
        "        final_df = df\n",
        "\n",
        "    # Fix paths relative to DATASET_DIR\n",
        "    old_root = Path('/content/drive/MyDrive/UCFClipFeatures')\n",
        "    final_df['path'] = final_df['path'].apply(lambda p: str(DATASET_DIR / Path(p).relative_to(old_root)))\n",
        "\n",
        "    # Save\n",
        "    final_df.drop(columns='video_id').to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "57aXEliJT-Q0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_lists(Args.test_list)\n",
        "update_lists(Args.train_list, limit_list=None)"
      ],
      "metadata": {
        "id": "Gc_UEDSqVmDB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "class LayerNorm(nn.LayerNorm):\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        orig_type = x.dtype\n",
        "        ret = super().forward(x.type(torch.float32))\n",
        "        return ret.type(orig_type)\n",
        "\n",
        "\n",
        "class QuickGELU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
        "        ]))\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "    def attention(self, x: torch.Tensor, padding_mask: torch.Tensor):\n",
        "        padding_mask = padding_mask.to(dtype=bool, device=x.device) if padding_mask is not None else None\n",
        "        self.attn_mask = self.attn_mask.to(device=x.device) if self.attn_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, key_padding_mask=padding_mask, attn_mask=self.attn_mask)[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, padding_mask = x\n",
        "        x = x + self.attention(self.ln_1(x), padding_mask)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return (x, padding_mask)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.layers = layers\n",
        "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.resblocks(x)\n",
        "\n",
        "\n",
        "class CLIPVAD(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_class: int,\n",
        "                 embed_dim: int,\n",
        "                 visual_length: int,\n",
        "                 visual_width: int,\n",
        "                 visual_head: int,\n",
        "                 visual_layers: int,\n",
        "                 attn_window: int,\n",
        "                 prompt_prefix: int,\n",
        "                 prompt_postfix: int,\n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_class = num_class\n",
        "        self.visual_length = visual_length\n",
        "        self.visual_width = visual_width\n",
        "        self.embed_dim = embed_dim\n",
        "        self.attn_window = attn_window\n",
        "        self.prompt_prefix = prompt_prefix\n",
        "        self.prompt_postfix = prompt_postfix\n",
        "        self.device = device\n",
        "\n",
        "        self.temporal = Transformer(\n",
        "            width=visual_width,\n",
        "            layers=visual_layers,\n",
        "            heads=visual_head,\n",
        "            attn_mask=self.build_attention_mask(self.attn_window)\n",
        "        )\n",
        "\n",
        "        width = int(visual_width / 2)\n",
        "        self.gc1 = GraphConvolution(visual_width, width, residual=True)\n",
        "        self.gc2 = GraphConvolution(width, width, residual=True)\n",
        "        self.gc3 = GraphConvolution(visual_width, width, residual=True)\n",
        "        self.gc4 = GraphConvolution(width, width, residual=True)\n",
        "        self.disAdj = DistanceAdj()\n",
        "        self.linear = nn.Linear(visual_width, visual_width)\n",
        "        self.gelu = QuickGELU()\n",
        "\n",
        "        self.mlp1 = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n",
        "        ]))\n",
        "        self.mlp2 = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n",
        "        ]))\n",
        "        self.classifier = nn.Linear(visual_width, 1)\n",
        "\n",
        "        self.clipmodel, _ = clip.load(\"ViT-B/16\", device)\n",
        "        for clip_param in self.clipmodel.parameters():\n",
        "            clip_param.requires_grad = False\n",
        "\n",
        "        self.frame_position_embeddings = nn.Embedding(visual_length, visual_width)\n",
        "        self.text_prompt_embeddings = nn.Embedding(77, self.embed_dim)\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.text_prompt_embeddings.weight, std=0.01)\n",
        "        nn.init.normal_(self.frame_position_embeddings.weight, std=0.01)\n",
        "\n",
        "    def build_attention_mask(self, attn_window):\n",
        "        # lazily create causal attention mask, with full attention between the vision tokens\n",
        "        # pytorch uses additive attention mask; fill with -inf\n",
        "        mask = torch.empty(self.visual_length, self.visual_length)\n",
        "        mask.fill_(float('-inf'))\n",
        "        for i in range(int(self.visual_length / attn_window)):\n",
        "            if (i + 1) * attn_window < self.visual_length:\n",
        "                mask[i * attn_window: (i + 1) * attn_window, i * attn_window: (i + 1) * attn_window] = 0\n",
        "            else:\n",
        "                mask[i * attn_window: self.visual_length, i * attn_window: self.visual_length] = 0\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def adj4(self, x, seq_len):\n",
        "        soft = nn.Softmax(1)\n",
        "        x2 = x.matmul(x.permute(0, 2, 1)) # B*T*T\n",
        "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = x_norm.matmul(x_norm.permute(0, 2, 1))\n",
        "        x2 = x2/(x_norm_x+1e-20)\n",
        "        output = torch.zeros_like(x2)\n",
        "        if seq_len is None:\n",
        "            for i in range(x.shape[0]):\n",
        "                tmp = x2[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n",
        "    def encode_video(self, images, padding_mask, lengths):\n",
        "        images = images.to(torch.float)\n",
        "        position_ids = torch.arange(self.visual_length, device=self.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(images.shape[0], -1)\n",
        "        frame_position_embeddings = self.frame_position_embeddings(position_ids)\n",
        "        frame_position_embeddings = frame_position_embeddings.permute(1, 0, 2)\n",
        "        images = images.permute(1, 0, 2) + frame_position_embeddings\n",
        "\n",
        "        x, _ = self.temporal((images, None))\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        adj = self.adj4(x, lengths)\n",
        "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
        "        x1_h = self.gelu(self.gc1(x, adj))\n",
        "        x2_h = self.gelu(self.gc3(x, disadj))\n",
        "\n",
        "        x1 = self.gelu(self.gc2(x1_h, adj))\n",
        "        x2 = self.gelu(self.gc4(x2_h, disadj))\n",
        "\n",
        "        x = torch.cat((x1, x2), 2)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def encode_textprompt(self, text):\n",
        "        word_tokens = clip.tokenize(text).to(self.device)\n",
        "        word_embedding = self.clipmodel.encode_token(word_tokens)\n",
        "        text_embeddings = self.text_prompt_embeddings(torch.arange(77).to(self.device)).unsqueeze(0).repeat([len(text), 1, 1])\n",
        "        text_tokens = torch.zeros(len(text), 77).to(self.device)\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            ind = torch.argmax(word_tokens[i], -1)\n",
        "            text_embeddings[i, 0] = word_embedding[i, 0]\n",
        "            text_embeddings[i, self.prompt_prefix + 1: self.prompt_prefix + ind] = word_embedding[i, 1: ind]\n",
        "            text_embeddings[i, self.prompt_prefix + ind + self.prompt_postfix] = word_embedding[i, ind]\n",
        "            text_tokens[i, self.prompt_prefix + ind + self.prompt_postfix] = word_tokens[i, ind]\n",
        "\n",
        "        text_features = self.clipmodel.encode_text(text_embeddings, text_tokens)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(self, visual, padding_mask, text, lengths):\n",
        "        visual_features = self.encode_video(visual, padding_mask, lengths)\n",
        "        logits1 = self.classifier(visual_features + self.mlp2(visual_features))\n",
        "\n",
        "        text_features_ori = self.encode_textprompt(text)\n",
        "\n",
        "        text_features = text_features_ori\n",
        "        logits_attn = logits1.permute(0, 2, 1)\n",
        "        visual_attn = logits_attn @ visual_features\n",
        "        visual_attn = visual_attn / visual_attn.norm(dim=-1, keepdim=True)\n",
        "        visual_attn = visual_attn.expand(visual_attn.shape[0], text_features_ori.shape[0], visual_attn.shape[2])\n",
        "        text_features = text_features_ori.unsqueeze(0)\n",
        "        text_features = text_features.expand(visual_attn.shape[0], text_features.shape[1], text_features.shape[2])\n",
        "        text_features = text_features + visual_attn\n",
        "        text_features = text_features + self.mlp1(text_features)\n",
        "\n",
        "        visual_features_norm = visual_features / visual_features.norm(dim=-1, keepdim=True)\n",
        "        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        text_features_norm = text_features_norm.permute(0, 2, 1)\n",
        "        logits2 = visual_features_norm @ text_features_norm.type(visual_features_norm.dtype) / 0.07\n",
        "\n",
        "        return text_features_ori, logits1, logits2, visual_features\n"
      ],
      "metadata": {
        "id": "2ODZ8AzzN3_M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Function\n",
        "def test(model, testdataloader, maxlen, prompt_text, gt, gtsegments, gtlabels, device):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    element_logits2_stack = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, item in enumerate(testdataloader):\n",
        "            visual = item[0].squeeze(0)\n",
        "            length = item[2]\n",
        "\n",
        "            length = int(length)\n",
        "            len_cur = length\n",
        "            if len_cur < maxlen:\n",
        "                visual = visual.unsqueeze(0)\n",
        "\n",
        "            visual = visual.to(device)\n",
        "\n",
        "            lengths = torch.zeros(int(length / maxlen) + 1)\n",
        "            for j in range(int(length / maxlen) + 1):\n",
        "                if j == 0 and length < maxlen:\n",
        "                    lengths[j] = length\n",
        "                elif j == 0 and length > maxlen:\n",
        "                    lengths[j] = maxlen\n",
        "                    length -= maxlen\n",
        "                elif length > maxlen:\n",
        "                    lengths[j] = maxlen\n",
        "                    length -= maxlen\n",
        "                else:\n",
        "                    lengths[j] = length\n",
        "            lengths = lengths.to(int)\n",
        "            padding_mask = get_batch_mask(lengths, maxlen).to(device)\n",
        "            _, logits1, logits2, _ = model(visual, padding_mask, prompt_text, lengths)\n",
        "            logits1 = logits1.reshape(logits1.shape[0] * logits1.shape[1], logits1.shape[2])\n",
        "            logits2 = logits2.reshape(logits2.shape[0] * logits2.shape[1], logits2.shape[2])\n",
        "            prob2 = (1 - logits2[0:len_cur].softmax(dim=-1)[:, 0].squeeze(-1))\n",
        "            prob1 = torch.sigmoid(logits1[0:len_cur].squeeze(-1))\n",
        "\n",
        "            if i == 0:\n",
        "                ap1 = prob1\n",
        "                ap2 = prob2\n",
        "                #ap3 = prob3\n",
        "            else:\n",
        "                ap1 = torch.cat([ap1, prob1], dim=0)\n",
        "                ap2 = torch.cat([ap2, prob2], dim=0)\n",
        "\n",
        "            element_logits2 = logits2[0:len_cur].softmax(dim=-1).detach().cpu().numpy()\n",
        "            element_logits2 = np.repeat(element_logits2, 16, 0)\n",
        "            element_logits2_stack.append(element_logits2)\n",
        "\n",
        "    ap1 = ap1.cpu().numpy()\n",
        "    ap2 = ap2.cpu().numpy()\n",
        "    ap1 = ap1.tolist()\n",
        "    ap2 = ap2.tolist()\n",
        "\n",
        "    ROC1 = roc_auc_score(gt, np.repeat(ap1, 16))\n",
        "    AP1 = average_precision_score(gt, np.repeat(ap1, 16))\n",
        "    ROC2 = roc_auc_score(gt, np.repeat(ap2, 16))\n",
        "    AP2 = average_precision_score(gt, np.repeat(ap2, 16))\n",
        "\n",
        "    print(\"AUC1: \", ROC1, \" AP1: \", AP1)\n",
        "    print(\"AUC2: \", ROC2, \" AP2:\", AP2)\n",
        "\n",
        "    dmap, iou = dmAP(element_logits2_stack, gtsegments, gtlabels, excludeNormal=False)\n",
        "    averageMAP = 0\n",
        "    for i in range(5):\n",
        "        print('mAP@{0:.1f} ={1:.2f}%'.format(iou[i], dmap[i]))\n",
        "        averageMAP += dmap[i]\n",
        "    averageMAP = averageMAP/(i+1)\n",
        "    print('average MAP: {:.2f}'.format(averageMAP))\n",
        "\n",
        "    return ROC1, AP1, averageMAP\n",
        "\n",
        "\n",
        "def run_test():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    args = Args()\n",
        "\n",
        "    label_map = dict({'Normal': 'Normal', 'Abuse': 'Abuse', 'Arrest': 'Arrest', 'Arson': 'Arson', 'Assault': 'Assault', 'Burglary': 'Burglary', 'Explosion': 'Explosion', 'Fighting': 'Fighting', 'RoadAccidents': 'RoadAccidents', 'Robbery': 'Robbery', 'Shooting': 'Shooting', 'Shoplifting': 'Shoplifting', 'Stealing': 'Stealing', 'Vandalism': 'Vandalism'})\n",
        "\n",
        "    testdataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n",
        "    testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    prompt_text = get_prompt_text(label_map)\n",
        "    gt = np.load(args.gt_path)\n",
        "    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n",
        "    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n",
        "\n",
        "    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n",
        "    model_param = torch.load(args.model_path)\n",
        "    model.load_state_dict(model_param)\n",
        "\n",
        "    test(model, testdataloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)"
      ],
      "metadata": {
        "id": "8LrMWNfSZ-RX",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Function\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TripletLoss, self).__init__()\n",
        "\n",
        "    def distance(self, x, y):\n",
        "        return torch.cdist(x, y, p=2)\n",
        "\n",
        "    def forward(self, feats, margin=100.0):\n",
        "        bs = feats.size(0)\n",
        "        n_feats = feats[:bs // 2]\n",
        "        a_feats = feats[bs // 2:]\n",
        "\n",
        "        # Shape: (N, N) and (N, A)\n",
        "        n_d = self.distance(n_feats, n_feats)\n",
        "        a_d = self.distance(n_feats, a_feats)\n",
        "\n",
        "        n_d_max, _ = torch.max(n_d, dim=0)         # shape: (N,)\n",
        "        a_d_min, _ = torch.min(a_d, dim=0)         # shape: (A,)\n",
        "\n",
        "        a_d_min = margin - a_d_min\n",
        "        a_d_min = torch.max(torch.zeros_like(a_d_min), a_d_min)  # element-wise clamp to >= 0\n",
        "\n",
        "        return torch.mean(n_d_max) + torch.mean(a_d_min)\n",
        "\n",
        "\n",
        "\n",
        "def CLASM(logits, labels, lengths, device):\n",
        "    instance_logits = torch.zeros(0).to(device)\n",
        "    labels = labels / torch.sum(labels, dim=1, keepdim=True)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True, dim=0)\n",
        "        instance_logits = torch.cat([instance_logits, torch.mean(tmp, 0, keepdim=True)], dim=0)\n",
        "\n",
        "    milloss = -torch.mean(torch.sum(labels * F.log_softmax(instance_logits, dim=1), dim=1), dim=0)\n",
        "    return milloss\n",
        "\n",
        "def CLAS2(logits, labels, lengths, device):\n",
        "    instance_logits = torch.zeros(0).to(device)\n",
        "    labels = 1 - labels[:, 0].reshape(labels.shape[0])\n",
        "    labels = labels.to(device)\n",
        "    logits = torch.sigmoid(logits).reshape(logits.shape[0], logits.shape[1])\n",
        "\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True)\n",
        "        tmp = torch.mean(tmp).view(1)\n",
        "        instance_logits = torch.cat([instance_logits, tmp], dim=0)\n",
        "\n",
        "    clsloss = F.binary_cross_entropy(instance_logits, labels)\n",
        "    return clsloss\n",
        "\n",
        "\n",
        "def train(model, normal_loader, anomaly_loader, testloader, args, label_map, device, experiment):\n",
        "    model.to(device)\n",
        "    gt = np.load(args.gt_path)\n",
        "    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n",
        "    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    scheduler = MultiStepLR(optimizer, args.scheduler_milestones, args.scheduler_rate)\n",
        "    triplet_loss_fn = TripletLoss().to(device)\n",
        "    prompt_text = get_prompt_text(label_map)\n",
        "    auc_best = 0\n",
        "    epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if args.use_checkpoint:\n",
        "        download_commet_asset(args.checkpoint_file)\n",
        "        checkpoint = torch.load(args.checkpoint_file, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        auc_best = checkpoint['auc']\n",
        "        print(\"checkpoint info:\")\n",
        "        print(\"epoch:\", epoch+1, \" auc:\", auc_best)\n",
        "\n",
        "    for e in range(args.max_epoch):\n",
        "        curr_epoch = e + 1\n",
        "        experiment.log_current_epoch(curr_epoch)\n",
        "        model.train()\n",
        "        loss_total1 = 0\n",
        "        loss_total2 = 0\n",
        "        normal_iter = iter(normal_loader)\n",
        "        anomaly_iter = iter(anomaly_loader)\n",
        "\n",
        "        for i in range(min(len(normal_loader), len(anomaly_loader))):\n",
        "            step = i * normal_loader.batch_size * 2\n",
        "            global_step += 1\n",
        "\n",
        "            normal_features, normal_label, normal_lengths = next(normal_iter)\n",
        "            anomaly_features, anomaly_label, anomaly_lengths = next(anomaly_iter)\n",
        "\n",
        "            visual_features = torch.cat([normal_features, anomaly_features], dim=0).to(device)\n",
        "            text_labels = list(normal_label) + list(anomaly_label)\n",
        "            feat_lengths = torch.cat([normal_lengths, anomaly_lengths], dim=0).to(device)\n",
        "            text_labels = get_batch_label(text_labels, prompt_text, label_map).to(device)\n",
        "\n",
        "            text_features, logits1, logits2, visual_feats = model(visual_features, None, prompt_text, feat_lengths)\n",
        "\n",
        "            loss1 = CLAS2(logits1, text_labels, feat_lengths, device)\n",
        "            loss_total1 += loss1.item()\n",
        "\n",
        "            loss2 = CLASM(logits2, text_labels, feat_lengths, device)\n",
        "            loss_total2 += loss2.item()\n",
        "\n",
        "            triplet_loss_val = triplet_loss_fn(visual_feats)\n",
        "\n",
        "            loss3 = torch.zeros(1).to(device)\n",
        "            text_feature_normal = text_features[0] / text_features[0].norm(dim=-1, keepdim=True)\n",
        "            for j in range(1, text_features.shape[0]):\n",
        "                text_feature_abr = text_features[j] / text_features[j].norm(dim=-1, keepdim=True)\n",
        "                loss3 += torch.abs(text_feature_normal @ text_feature_abr)\n",
        "            loss3 = loss3 / 13 * 1e-1\n",
        "\n",
        "            loss = loss1 + loss2 + loss3 + 0.01 * triplet_loss_val\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss1 = loss_total1 / (i + 1)\n",
        "            avg_loss2 = loss_total2 / (i + 1)\n",
        "            loss3_val = loss3.item()\n",
        "\n",
        "            # Comet logging (per step)\n",
        "            experiment.log_metric(\"loss1\", avg_loss1, step=global_step)\n",
        "            experiment.log_metric(\"loss2\", avg_loss2, step=global_step)\n",
        "            experiment.log_metric(\"loss3\", loss3_val, step=global_step)\n",
        "            experiment.log_metric(\"loss_triplet\", triplet_loss_val.item(), step=global_step)\n",
        "\n",
        "\n",
        "            if step % 1280 == 0 and step != 0:\n",
        "                print(f'epoch: {curr_epoch} | step: {step} | loss1: {avg_loss1:.4f} | loss2: {avg_loss2:.4f} | loss3: {loss3_val:.4f}')\n",
        "\n",
        "                # Evaluate\n",
        "                AUC, AP, averageMAP = test(model, testloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)\n",
        "                experiment.log_metric(\"AUC\", AUC, step=global_step)\n",
        "                experiment.log_metric(\"Average_MAP\", averageMAP, step=global_step)\n",
        "\n",
        "                if AUC > auc_best:\n",
        "                    auc_best = AUC\n",
        "                    experiment.log_metric(\"Best_AUC\", auc_best, step=global_step)\n",
        "\n",
        "                    checkpoint = {\n",
        "                        'epoch': curr_epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'auc': auc_best\n",
        "                    }\n",
        "                    torch.save(checkpoint, args.checkpoint_path)\n",
        "\n",
        "                    # Log checkpoint to Comet\n",
        "                    experiment.log_asset(file_data=args.checkpoint_path, file_name=args.checkpoint_file, overwrite=True)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        experiment.log_metric(\"epoch\", curr_epoch, step=global_step)\n",
        "\n",
        "        # Save current model weights separately\n",
        "        checkpoint_dir = os.path.dirname(args.checkpoint_path)\n",
        "        save_path = os.path.join(checkpoint_dir, 'model_cur.pth')\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        # Reload best checkpoint before next epoch\n",
        "        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Save and log final model weights\n",
        "    checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n",
        "    torch.save(checkpoint['model_state_dict'], args.model_path)\n",
        "    experiment.log_model(\"final_model\", args.model_path)\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    #torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def run_train(experiment):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    args = Args()\n",
        "    setup_seed(args.seed)\n",
        "\n",
        "    label_map = dict({'Normal': 'normal', 'Abuse': 'abuse', 'Arrest': 'arrest', 'Arson': 'arson', 'Assault': 'assault', 'Burglary': 'burglary', 'Explosion': 'explosion', 'Fighting': 'fighting', 'RoadAccidents': 'roadAccidents', 'Robbery': 'robbery', 'Shooting': 'shooting', 'Shoplifting': 'shoplifting', 'Stealing': 'stealing', 'Vandalism': 'vandalism'})\n",
        "\n",
        "    normal_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, True)\n",
        "    normal_loader = DataLoader(normal_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "    anomaly_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, False)\n",
        "    anomaly_loader = DataLoader(anomaly_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    test_dataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n",
        "\n",
        "    train(model, normal_loader, anomaly_loader, test_loader, args, label_map, device, experiment)\n",
        "\n"
      ],
      "metadata": {
        "id": "eyaLSBFMpdWk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = comet_ml.Experiment(\n",
        "                  api_key=COMET_API_KEY,\n",
        "                  project_name=COMET_PROJECT_NAME)\n",
        "\n",
        "with experiment.train():\n",
        "  run_train(experiment)\n",
        "\n",
        "experiment.end()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBtt-FAfgY2A",
        "outputId": "ccce6dc2-c214-4e60-97eb-3ca31b4a3aa1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hammad-ali/vad-clip-notebook/b5386d58c9644dbc9604c2d45de89073\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  715M  100  715M    0     0  38.7M      0  0:00:18  0:00:18 --:--:-- 39.0M\n",
            "checkpoint info:\n",
            "epoch: 8  auc: 0.8830567626634271\n",
            "epoch: 1 | step: 1280 | loss1: 0.0457 | loss2: 0.6221 | loss3: 0.0299\n",
            "AUC1:  0.8825239827623643  AP1:  0.3249366210197735\n",
            "AUC2:  0.8663374816300369  AP2: 0.28601921057395086\n",
            "mAP@0.1 =13.51%\n",
            "mAP@0.2 =11.45%\n",
            "mAP@0.3 =6.18%\n",
            "mAP@0.4 =3.86%\n",
            "mAP@0.5 =3.61%\n",
            "average MAP: 7.72\n"
          ]
        }
      ]
    }
  ]
}