{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aNpPTDBgNr1nK63ztQXatuHhXMJLsGAa","authorship_tag":"ABX9TyOYuktlsmas4TXQAZDYX+kp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hammad-ali1/vad_clip_colab_notebook/blob/main/vad_clip_notebook.ipynb)\n"],"metadata":{"id":"hjJEkZzImzOk"}},{"cell_type":"code","source":["!pip install ftfy\n","!pip install comet_ml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DfMHfDVZPB8","executionInfo":{"status":"ok","timestamp":1752404504038,"user_tz":-300,"elapsed":16995,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"67c686cd-efdb-4a05-ebf4-25cb7ea9c9cf","collapsed":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Collecting comet_ml\n","  Downloading comet_ml-3.49.11-py3-none-any.whl.metadata (4.1 kB)\n","Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n","  Downloading dulwich-0.23.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n","Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n","  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (4.24.0)\n","Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (5.9.5)\n","Collecting python-box<7.0.0 (from comet_ml)\n","  Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n","Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.0.0)\n","Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.32.3)\n","Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (13.9.4)\n","Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.10.0)\n","Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.32.0)\n","Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from comet_ml) (3.20.1)\n","Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.4.0)\n","Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.17.2)\n","Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (3.1.1)\n","Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n","  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.26.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (2025.7.9)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.14.1)\n","Downloading comet_ml-3.49.11-py3-none-any.whl (727 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.1/727.1 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dulwich-0.23.2-cp311-cp311-manylinux_2_28_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n","Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n","Installing collected packages: everett, python-box, dulwich, configobj, comet_ml\n","  Attempting uninstall: python-box\n","    Found existing installation: python-box 7.3.2\n","    Uninstalling python-box-7.3.2:\n","      Successfully uninstalled python-box-7.3.2\n","Successfully installed comet_ml-3.49.11 configobj-5.0.9 dulwich-0.23.2 everett-3.1.0 python-box-6.1.0\n"]}]},{"cell_type":"code","source":["#@title Import Modules\n","import os\n","from pathlib import Path\n","from collections import OrderedDict\n","import os\n","from pathlib import Path\n","\n","import comet_ml\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import average_precision_score, roc_auc_score\n","\n","from clip import clip\n","from utils.dataset import UCFDataset\n","from utils.layers import GraphConvolution, DistanceAdj\n","from utils.tools import get_batch_mask, get_prompt_text\n","from utils.ucf_detectionMAP import getDetectionMAP as dmAP"],"metadata":{"id":"F_pOyYg7Ogdd","executionInfo":{"status":"ok","timestamp":1752404532405,"user_tz":-300,"elapsed":27875,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["DRIVE_DATASET_DIR = Path('/content/drive/MyDrive/UCFClipFeatures')\n","\n","if os.path.ismount('/content/drive') and DRIVE_DATASET_DIR.exists():\n","  DATASET_DIR = DRIVE_DATASET_DIR\n","else:\n","  !gdown 1DOkXHObwEGqmU3c9-J1gCIyVXiTMFfC5\n","  !unzip /content/UCFClipFeatures.zip -d /content\n","  !rm /content/UCFClipFeatures.zip\n","  DATASET_DIR = Path('/content/UCFClipFeatures')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXPCB6SdOqzk","executionInfo":{"status":"ok","timestamp":1752401230724,"user_tz":-300,"elapsed":151402,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"e169cb83-c10d-46dd-c778-fbe1376fd7bc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1DOkXHObwEGqmU3c9-J1gCIyVXiTMFfC5\n","From (redirected): https://drive.google.com/uc?id=1DOkXHObwEGqmU3c9-J1gCIyVXiTMFfC5&confirm=t&uuid=8e18d182-a7b6-4973-9640-19d56b7cdb39\n","To: /content/UCFClipFeatures.zip\n","100% 13.0G/13.0G [02:28<00:00, 87.5MB/s]\n"]}]},{"cell_type":"code","source":["#@title Pull Latest Repo\n","from google.colab import userdata\n","\n","gh_token = userdata.get('vad_clip_gh_token')\n","\n","REPO_NAME = \"vad_clip_colab_notebook\"\n","REPO_PATH = Path(\"/content\") / REPO_NAME\n","\n","if gh_token:\n","  GITHUB_URL = f\"https://{gh_token}@github.com/hammad-ali1/{REPO_NAME}.git\"\n","else:\n","  GITHUB_URL = f'https://github.com/hammad-ali1/{REPO_NAME}'\n","\n","!rm -rf {REPO_PATH}\n","!git clone {GITHUB_URL}\n","\n","!cp -r {REPO_PATH}/* /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3JvbCrKXHfc","executionInfo":{"status":"ok","timestamp":1752402220950,"user_tz":-300,"elapsed":4399,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"226013ff-7cec-4f0e-bd59-b84bd6425a39"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vad_clip_colab_notebook'...\n","remote: Enumerating objects: 64, done.\u001b[K\n","remote: Counting objects: 100% (64/64), done.\u001b[K\n","remote: Compressing objects: 100% (52/52), done.\u001b[K\n","remote: Total 64 (delta 15), reused 51 (delta 10), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (64/64), 1.63 MiB | 12.29 MiB/s, done.\n","Resolving deltas: 100% (15/15), done.\n"]}]},{"cell_type":"code","source":["#@title Utils\n","def push_notebook_to_gh():\n","  commit_msg = input(\"Commit message: \")\n","  NOTEBOOK_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/vad_clip/vad_clip_notebook.ipynb\")\n","  !cp \"{NOTEBOOK_PATH}\" {REPO_PATH}\n","  !cd {REPO_PATH} && git config user.name \"Hammad Ali\"\n","  !cd {REPO_PATH} && git config user.email \"hammad.a22002@gmail.com\"\n","\n","  # Add, commit and push\n","  !cd {REPO_PATH} && git add .\n","  !cd {REPO_PATH} && git commit -m \"{commit_msg}\"\n","  !cd {REPO_PATH} && git push"],"metadata":{"id":"CAuhT-UbJJ6V","executionInfo":{"status":"ok","timestamp":1752393350780,"user_tz":-300,"elapsed":17,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"cellView":"form"},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class Args:\n","    seed = 234\n","\n","    embed_dim = 512\n","    visual_length = 256\n","    visual_width = 512\n","    visual_head = 1\n","    visual_layers = 2\n","    attn_window = 8\n","    prompt_prefix = 10\n","    prompt_postfix = 10\n","    classes_num = 14\n","\n","    max_epoch = 10\n","    model_path = \"model/model_cur.pth\"\n","    use_checkpoint = False\n","    checkpoint_path = \"model/checkpoint.pth\"\n","    batch_size = 64\n","    train_list = 'list/ucf_CLIP_rgb.csv'\n","    test_list = 'list/ucf_CLIP_rgbtest.csv'\n","    gt_path = 'list/gt_ucf.npy'\n","    gt_segment_path = 'list/gt_segment_ucf.npy'\n","    gt_label_path = 'list/gt_label_ucf.npy'\n","\n","    lr = 2e-5\n","    scheduler_rate = 0.1\n","    scheduler_milestones = [4, 8]\n"],"metadata":{"id":"0VPhYpQPbBSN","executionInfo":{"status":"ok","timestamp":1752404618879,"user_tz":-300,"elapsed":17,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def update_lists(file_path):\n","  df = pd.read_csv(file_path)  # replace with your actual file path\n","  old_root = Path('/content/drive/MyDrive/UCFClipFeatures')\n","  df['path'] = df['path'].apply(lambda p: str(DATASET_DIR / Path(p).relative_to(old_root)))\n","  df.to_csv(file_path, index=False)\n","\n","update_lists(Args.test_list)\n","update_lists(Args.train_list)"],"metadata":{"id":"57aXEliJT-Q0","executionInfo":{"status":"ok","timestamp":1752404653831,"user_tz":-300,"elapsed":272,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["COMET_API_KEY = userdata.get('COMET_API_KEY')\n","experiment = comet_ml.Experiment(\n","                  api_key=COMET_API_KEY,\n","                  project_name=\"vad_clip_notebook\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFec1F6E47IJ","executionInfo":{"status":"ok","timestamp":1752396188203,"user_tz":-300,"elapsed":3233,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"bf73e5bb-c4ef-4f8a-d954-10e36a8fe567"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n","\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hammad-ali/vad-clip-notebook/fc9dc2fc117e4b319816e11f5b5f6fd5\n","\n"]}]},{"cell_type":"code","source":["#@title Model\n","class LayerNorm(nn.LayerNorm):\n","\n","    def forward(self, x: torch.Tensor):\n","        orig_type = x.dtype\n","        ret = super().forward(x.type(torch.float32))\n","        return ret.type(orig_type)\n","\n","\n","class QuickGELU(nn.Module):\n","    def forward(self, x: torch.Tensor):\n","        return x * torch.sigmoid(1.702 * x)\n","\n","\n","class ResidualAttentionBlock(nn.Module):\n","    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n","        super().__init__()\n","\n","        self.attn = nn.MultiheadAttention(d_model, n_head)\n","        self.ln_1 = LayerNorm(d_model)\n","        self.mlp = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n","        ]))\n","        self.ln_2 = LayerNorm(d_model)\n","        self.attn_mask = attn_mask\n","\n","    def attention(self, x: torch.Tensor, padding_mask: torch.Tensor):\n","        padding_mask = padding_mask.to(dtype=bool, device=x.device) if padding_mask is not None else None\n","        self.attn_mask = self.attn_mask.to(device=x.device) if self.attn_mask is not None else None\n","        return self.attn(x, x, x, need_weights=False, key_padding_mask=padding_mask, attn_mask=self.attn_mask)[0]\n","\n","    def forward(self, x):\n","        x, padding_mask = x\n","        x = x + self.attention(self.ln_1(x), padding_mask)\n","        x = x + self.mlp(self.ln_2(x))\n","        return (x, padding_mask)\n","\n","\n","# class Transformer(nn.Module):\n","#     def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n","#         super().__init__()\n","#         self.width = width\n","#         self.layers = layers\n","#         self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n","\n","#     def forward(self, x: torch.Tensor):\n","#         return self.resblocks(x)\n","\n","# Full Attention Transformer\n","class Transformer(nn.Module):\n","    def __init__(self, width: int, layers: int, heads: int):\n","        super().__init__()\n","        self.width = width\n","        self.layers = layers\n","        self.resblocks = nn.Sequential(*[\n","            ResidualAttentionBlock(width, heads, attn_mask=None) for _ in range(layers)\n","        ])\n","\n","    def forward(self, x: torch.Tensor):\n","        return self.resblocks(x)\n","\n","\n","class CLIPVAD(nn.Module):\n","    def __init__(self,\n","                 num_class: int,\n","                 embed_dim: int,\n","                 visual_length: int,\n","                 visual_width: int,\n","                 visual_head: int,\n","                 visual_layers: int,\n","                 attn_window: int,\n","                 prompt_prefix: int,\n","                 prompt_postfix: int,\n","                 device):\n","        super().__init__()\n","\n","        self.num_class = num_class\n","        self.visual_length = visual_length\n","        self.visual_width = visual_width\n","        self.embed_dim = embed_dim\n","        self.attn_window = attn_window\n","        self.prompt_prefix = prompt_prefix\n","        self.prompt_postfix = prompt_postfix\n","        self.device = device\n","\n","        self.temporal = Transformer(\n","            width=visual_width,\n","            layers=visual_layers,\n","            heads=visual_head,\n","            # attn_mask=self.build_attention_mask(self.attn_window)\n","        )\n","\n","        width = int(visual_width / 2)\n","        # self.gc1 = GraphConvolution(visual_width, width, residual=True)\n","        # self.gc2 = GraphConvolution(width, width, residual=True)\n","        # self.gc3 = GraphConvolution(visual_width, width, residual=True)\n","        # self.gc4 = GraphConvolution(width, width, residual=True)\n","        # self.disAdj = DistanceAdj()\n","        self.linear = nn.Linear(visual_width, visual_width)\n","        self.gelu = QuickGELU()\n","\n","        self.mlp1 = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n","        ]))\n","        self.mlp2 = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n","        ]))\n","        self.classifier = nn.Linear(visual_width, 1)\n","\n","        self.clipmodel, _ = clip.load(\"ViT-B/16\", device)\n","        for clip_param in self.clipmodel.parameters():\n","            clip_param.requires_grad = False\n","\n","        self.frame_position_embeddings = nn.Embedding(visual_length, visual_width)\n","        self.text_prompt_embeddings = nn.Embedding(77, self.embed_dim)\n","\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        nn.init.normal_(self.text_prompt_embeddings.weight, std=0.01)\n","        nn.init.normal_(self.frame_position_embeddings.weight, std=0.01)\n","\n","    def build_attention_mask(self, attn_window):\n","        # lazily create causal attention mask, with full attention between the vision tokens\n","        # pytorch uses additive attention mask; fill with -inf\n","        mask = torch.empty(self.visual_length, self.visual_length)\n","        mask.fill_(float('-inf'))\n","        for i in range(int(self.visual_length / attn_window)):\n","            if (i + 1) * attn_window < self.visual_length:\n","                mask[i * attn_window: (i + 1) * attn_window, i * attn_window: (i + 1) * attn_window] = 0\n","            else:\n","                mask[i * attn_window: self.visual_length, i * attn_window: self.visual_length] = 0\n","\n","        return mask\n","\n","    def adj4(self, x, seq_len):\n","        soft = nn.Softmax(1)\n","        x2 = x.matmul(x.permute(0, 2, 1)) # B*T*T\n","        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n","        x_norm_x = x_norm.matmul(x_norm.permute(0, 2, 1))\n","        x2 = x2/(x_norm_x+1e-20)\n","        output = torch.zeros_like(x2)\n","        if seq_len is None:\n","            for i in range(x.shape[0]):\n","                tmp = x2[i]\n","                adj2 = tmp\n","                adj2 = F.threshold(adj2, 0.7, 0)\n","                adj2 = soft(adj2)\n","                output[i] = adj2\n","        else:\n","            for i in range(len(seq_len)):\n","                tmp = x2[i, :seq_len[i], :seq_len[i]]\n","                adj2 = tmp\n","                adj2 = F.threshold(adj2, 0.7, 0)\n","                adj2 = soft(adj2)\n","                output[i, :seq_len[i], :seq_len[i]] = adj2\n","\n","        return output\n","\n","    # def encode_video(self, images, padding_mask, lengths):\n","    #     images = images.to(torch.float)\n","    #     position_ids = torch.arange(self.visual_length, device=self.device)\n","    #     position_ids = position_ids.unsqueeze(0).expand(images.shape[0], -1)\n","    #     frame_position_embeddings = self.frame_position_embeddings(position_ids)\n","    #     frame_position_embeddings = frame_position_embeddings.permute(1, 0, 2)\n","    #     images = images.permute(1, 0, 2) + frame_position_embeddings\n","\n","    #     x, _ = self.temporal((images, None))\n","    #     x = x.permute(1, 0, 2)\n","\n","    #     adj = self.adj4(x, lengths)\n","    #     disadj = self.disAdj(x.shape[0], x.shape[1])\n","    #     x1_h = self.gelu(self.gc1(x, adj))\n","    #     x2_h = self.gelu(self.gc3(x, disadj))\n","\n","    #     x1 = self.gelu(self.gc2(x1_h, adj))\n","    #     x2 = self.gelu(self.gc4(x2_h, disadj))\n","\n","    #     x = torch.cat((x1, x2), 2)\n","    #     x = self.linear(x)\n","\n","    #     return x\n","\n","    def encode_video(self, images, padding_mask, lengths):\n","      images = images.to(torch.float)\n","\n","      position_ids = torch.arange(self.visual_length, device=self.device)\n","      position_ids = position_ids.unsqueeze(0).expand(images.shape[0], -1)\n","      frame_position_embeddings = self.frame_position_embeddings(position_ids)\n","      frame_position_embeddings = frame_position_embeddings.permute(1, 0, 2)\n","\n","      # Add positional encoding\n","      images = images.permute(1, 0, 2) + frame_position_embeddings  # Shape: [T, B, C]\n","\n","      # Global attention transformer over entire sequence\n","      x, _ = self.temporal((images, None))  # Output shape: [T, B, C]\n","      x = x.permute(1, 0, 2)  # [B, T, C]\n","\n","      return x  # No GCN applied\n","\n","    def encode_textprompt(self, text):\n","        word_tokens = clip.tokenize(text).to(self.device)\n","        word_embedding = self.clipmodel.encode_token(word_tokens)\n","        text_embeddings = self.text_prompt_embeddings(torch.arange(77).to(self.device)).unsqueeze(0).repeat([len(text), 1, 1])\n","        text_tokens = torch.zeros(len(text), 77).to(self.device)\n","\n","        for i in range(len(text)):\n","            ind = torch.argmax(word_tokens[i], -1)\n","            text_embeddings[i, 0] = word_embedding[i, 0]\n","            text_embeddings[i, self.prompt_prefix + 1: self.prompt_prefix + ind] = word_embedding[i, 1: ind]\n","            text_embeddings[i, self.prompt_prefix + ind + self.prompt_postfix] = word_embedding[i, ind]\n","            text_tokens[i, self.prompt_prefix + ind + self.prompt_postfix] = word_tokens[i, ind]\n","\n","        text_features = self.clipmodel.encode_text(text_embeddings, text_tokens)\n","\n","        return text_features\n","\n","    def forward(self, visual, padding_mask, text, lengths):\n","        visual_features = self.encode_video(visual, padding_mask, lengths)\n","        logits1 = self.classifier(visual_features + self.mlp2(visual_features))\n","\n","        text_features_ori = self.encode_textprompt(text)\n","\n","        text_features = text_features_ori\n","        logits_attn = logits1.permute(0, 2, 1)\n","        visual_attn = logits_attn @ visual_features\n","        visual_attn = visual_attn / visual_attn.norm(dim=-1, keepdim=True)\n","        visual_attn = visual_attn.expand(visual_attn.shape[0], text_features_ori.shape[0], visual_attn.shape[2])\n","        text_features = text_features_ori.unsqueeze(0)\n","        text_features = text_features.expand(visual_attn.shape[0], text_features.shape[1], text_features.shape[2])\n","        text_features = text_features + visual_attn\n","        text_features = text_features + self.mlp1(text_features)\n","\n","        visual_features_norm = visual_features / visual_features.norm(dim=-1, keepdim=True)\n","        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n","        text_features_norm = text_features_norm.permute(0, 2, 1)\n","        logits2 = visual_features_norm @ text_features_norm.type(visual_features_norm.dtype) / 0.07\n","\n","        return text_features_ori, logits1, logits2\n"],"metadata":{"id":"2ODZ8AzzN3_M","executionInfo":{"status":"ok","timestamp":1752396229215,"user_tz":-300,"elapsed":38,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#@title Test Function\n","def test(model, testdataloader, maxlen, prompt_text, gt, gtsegments, gtlabels, device):\n","\n","    model.to(device)\n","    model.eval()\n","\n","    element_logits2_stack = []\n","\n","    with torch.no_grad():\n","        for i, item in enumerate(testdataloader):\n","            visual = item[0].squeeze(0)\n","            length = item[2]\n","\n","            length = int(length)\n","            len_cur = length\n","            if len_cur < maxlen:\n","                visual = visual.unsqueeze(0)\n","\n","            visual = visual.to(device)\n","\n","            lengths = torch.zeros(int(length / maxlen) + 1)\n","            for j in range(int(length / maxlen) + 1):\n","                if j == 0 and length < maxlen:\n","                    lengths[j] = length\n","                elif j == 0 and length > maxlen:\n","                    lengths[j] = maxlen\n","                    length -= maxlen\n","                elif length > maxlen:\n","                    lengths[j] = maxlen\n","                    length -= maxlen\n","                else:\n","                    lengths[j] = length\n","            lengths = lengths.to(int)\n","            padding_mask = get_batch_mask(lengths, maxlen).to(device)\n","            _, logits1, logits2 = model(visual, padding_mask, prompt_text, lengths)\n","            logits1 = logits1.reshape(logits1.shape[0] * logits1.shape[1], logits1.shape[2])\n","            logits2 = logits2.reshape(logits2.shape[0] * logits2.shape[1], logits2.shape[2])\n","            prob2 = (1 - logits2[0:len_cur].softmax(dim=-1)[:, 0].squeeze(-1))\n","            prob1 = torch.sigmoid(logits1[0:len_cur].squeeze(-1))\n","\n","            if i == 0:\n","                ap1 = prob1\n","                ap2 = prob2\n","                #ap3 = prob3\n","            else:\n","                ap1 = torch.cat([ap1, prob1], dim=0)\n","                ap2 = torch.cat([ap2, prob2], dim=0)\n","\n","            element_logits2 = logits2[0:len_cur].softmax(dim=-1).detach().cpu().numpy()\n","            element_logits2 = np.repeat(element_logits2, 16, 0)\n","            element_logits2_stack.append(element_logits2)\n","\n","    ap1 = ap1.cpu().numpy()\n","    ap2 = ap2.cpu().numpy()\n","    ap1 = ap1.tolist()\n","    ap2 = ap2.tolist()\n","\n","    ROC1 = roc_auc_score(gt, np.repeat(ap1, 16))\n","    AP1 = average_precision_score(gt, np.repeat(ap1, 16))\n","    ROC2 = roc_auc_score(gt, np.repeat(ap2, 16))\n","    AP2 = average_precision_score(gt, np.repeat(ap2, 16))\n","\n","    print(\"AUC1: \", ROC1, \" AP1: \", AP1)\n","    print(\"AUC2: \", ROC2, \" AP2:\", AP2)\n","\n","    dmap, iou = dmAP(element_logits2_stack, gtsegments, gtlabels, excludeNormal=False)\n","    averageMAP = 0\n","    for i in range(5):\n","        print('mAP@{0:.1f} ={1:.2f}%'.format(iou[i], dmap[i]))\n","        averageMAP += dmap[i]\n","    averageMAP = averageMAP/(i+1)\n","    print('average MAP: {:.2f}'.format(averageMAP))\n","\n","    return ROC1, AP1\n","\n","\n","def run_test():\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args = Args()\n","\n","    label_map = dict({'Normal': 'Normal', 'Abuse': 'Abuse', 'Arrest': 'Arrest', 'Arson': 'Arson', 'Assault': 'Assault', 'Burglary': 'Burglary', 'Explosion': 'Explosion', 'Fighting': 'Fighting', 'RoadAccidents': 'RoadAccidents', 'Robbery': 'Robbery', 'Shooting': 'Shooting', 'Shoplifting': 'Shoplifting', 'Stealing': 'Stealing', 'Vandalism': 'Vandalism'})\n","\n","    testdataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n","    testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)\n","\n","    prompt_text = get_prompt_text(label_map)\n","    gt = np.load(args.gt_path)\n","    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n","    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n","\n","    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n","    model_param = torch.load(args.model_path)\n","    model.load_state_dict(model_param)\n","\n","    test(model, testdataloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)"],"metadata":{"id":"8LrMWNfSZ-RX","cellView":"form","executionInfo":{"status":"ok","timestamp":1752396213126,"user_tz":-300,"elapsed":29,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#@title Train Function\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import MultiStepLR\n","import numpy as np\n","import random\n","import os\n","\n","from utils.dataset import UCFDataset\n","from utils.tools import get_prompt_text, get_batch_label\n","\n","def CLASM(logits, labels, lengths, device):\n","    instance_logits = torch.zeros(0).to(device)\n","    labels = labels / torch.sum(labels, dim=1, keepdim=True)\n","    labels = labels.to(device)\n","\n","    for i in range(logits.shape[0]):\n","        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True, dim=0)\n","        instance_logits = torch.cat([instance_logits, torch.mean(tmp, 0, keepdim=True)], dim=0)\n","\n","    milloss = -torch.mean(torch.sum(labels * F.log_softmax(instance_logits, dim=1), dim=1), dim=0)\n","    return milloss\n","\n","def CLAS2(logits, labels, lengths, device):\n","    instance_logits = torch.zeros(0).to(device)\n","    labels = 1 - labels[:, 0].reshape(labels.shape[0])\n","    labels = labels.to(device)\n","    logits = torch.sigmoid(logits).reshape(logits.shape[0], logits.shape[1])\n","\n","    for i in range(logits.shape[0]):\n","        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True)\n","        tmp = torch.mean(tmp).view(1)\n","        instance_logits = torch.cat([instance_logits, tmp], dim=0)\n","\n","    clsloss = F.binary_cross_entropy(instance_logits, labels)\n","    return clsloss\n","\n","\n","import os\n","import torch\n","import numpy as np\n","from torch.optim.lr_scheduler import MultiStepLR\n","from pathlib import Path\n","\n","def train(model, normal_loader, anomaly_loader, testloader, args, label_map, device):\n","    model.to(device)\n","    gt = np.load(args.gt_path)\n","    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n","    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n","    scheduler = MultiStepLR(optimizer, args.scheduler_milestones, args.scheduler_rate)\n","    prompt_text = get_prompt_text(label_map)\n","    ap_best = 0\n","    epoch = 0\n","\n","    if args.use_checkpoint and Path(args.checkpoint_path).exists():\n","        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        ap_best = checkpoint['ap']\n","        print(\"checkpoint info:\")\n","        print(\"epoch:\", epoch+1, \" ap:\", ap_best)\n","\n","    for e in range(args.max_epoch):\n","        model.train()\n","        loss_total1 = 0\n","        loss_total2 = 0\n","        normal_iter = iter(normal_loader)\n","        anomaly_iter = iter(anomaly_loader)\n","\n","        for i in range(min(len(normal_loader), len(anomaly_loader))):\n","            step = i * normal_loader.batch_size * 2\n","\n","            normal_features, normal_label, normal_lengths = next(normal_iter)\n","            anomaly_features, anomaly_label, anomaly_lengths = next(anomaly_iter)\n","\n","            visual_features = torch.cat([normal_features, anomaly_features], dim=0).to(device)\n","            text_labels = list(normal_label) + list(anomaly_label)\n","            feat_lengths = torch.cat([normal_lengths, anomaly_lengths], dim=0).to(device)\n","            text_labels = get_batch_label(text_labels, prompt_text, label_map).to(device)\n","\n","            text_features, logits1, logits2 = model(visual_features, None, prompt_text, feat_lengths)\n","\n","            loss1 = CLAS2(logits1, text_labels, feat_lengths, device)\n","            loss_total1 += loss1.item()\n","\n","            loss2 = CLASM(logits2, text_labels, feat_lengths, device)\n","            loss_total2 += loss2.item()\n","\n","            loss3 = torch.zeros(1).to(device)\n","            text_feature_normal = text_features[0] / text_features[0].norm(dim=-1, keepdim=True)\n","            for j in range(1, text_features.shape[0]):\n","                text_feature_abr = text_features[j] / text_features[j].norm(dim=-1, keepdim=True)\n","                loss3 += torch.abs(text_feature_normal @ text_feature_abr)\n","            loss3 = loss3 / 13 * 1e-1\n","\n","            loss = loss1 + loss2 + loss3\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            avg_loss1 = loss_total1 / (i + 1)\n","            avg_loss2 = loss_total2 / (i + 1)\n","            loss3_val = loss3.item()\n","\n","            # Comet logging (per step)\n","            experiment.log_metric(\"loss1\", avg_loss1, step=step)\n","            experiment.log_metric(\"loss2\", avg_loss2, step=step)\n","            experiment.log_metric(\"loss3\", loss3_val, step=step)\n","\n","            if step % 1280 == 0 and step != 0:\n","                print(f'epoch: {e+1} | step: {step} | loss1: {avg_loss1:.4f} | loss2: {avg_loss2:.4f} | loss3: {loss3_val:.4f}')\n","\n","                # Evaluate\n","                AUC, AP = test(model, testloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)\n","                AP = AUC\n","                experiment.log_metric(\"AP\", AP, step=step)\n","\n","                if AP > ap_best:\n","                    ap_best = AP\n","                    experiment.log_metric(\"Best_AP\", ap_best, step=step)\n","\n","                    checkpoint = {\n","                        'epoch': e,\n","                        'model_state_dict': model.state_dict(),\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        'ap': ap_best\n","                    }\n","                    torch.save(checkpoint, args.checkpoint_path)\n","\n","                    # Log checkpoint to Comet\n","                    experiment.log_asset(file_data=args.checkpoint_path, file_name=\"best_checkpoint.pth\")\n","\n","        scheduler.step()\n","\n","        # Save current model weights separately\n","        checkpoint_dir = os.path.dirname(args.checkpoint_path)\n","        save_path = os.path.join(checkpoint_dir, 'model_cur.pth')\n","        torch.save(model.state_dict(), save_path)\n","\n","        # Reload best checkpoint before next epoch\n","        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # Save and log final model weights\n","    checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n","    torch.save(checkpoint['model_state_dict'], args.model_path)\n","    experiment.log_model(\"final_model\", args.model_path)\n","\n","\n","def setup_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    #torch.backends.cudnn.deterministic = True\n","\n","def run_train():\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args = Args()\n","    setup_seed(args.seed)\n","\n","    label_map = dict({'Normal': 'normal', 'Abuse': 'abuse', 'Arrest': 'arrest', 'Arson': 'arson', 'Assault': 'assault', 'Burglary': 'burglary', 'Explosion': 'explosion', 'Fighting': 'fighting', 'RoadAccidents': 'roadAccidents', 'Robbery': 'robbery', 'Shooting': 'shooting', 'Shoplifting': 'shoplifting', 'Stealing': 'stealing', 'Vandalism': 'vandalism'})\n","\n","    normal_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, True)\n","    normal_loader = DataLoader(normal_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n","    anomaly_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, False)\n","    anomaly_loader = DataLoader(anomaly_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n","\n","    test_dataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n","\n","    train(model, normal_loader, anomaly_loader, test_loader, args, label_map, device)\n"],"metadata":{"id":"eyaLSBFMpdWk","executionInfo":{"status":"ok","timestamp":1752396210763,"user_tz":-300,"elapsed":397,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["run_train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBtt-FAfgY2A","outputId":"b92b96f1-6bf6-48a0-cfd6-3fd8eb9f950f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1 | step: 1280 | loss1: 0.5671 | loss2: 2.4711 | loss3: 0.0980\n","AUC1:  0.7868346428338847  AP1:  0.17884024646622185\n","AUC2:  0.7446548214199078  AP2: 0.1755740481650378\n","mAP@0.1 =2.69%\n","mAP@0.2 =2.03%\n","mAP@0.3 =1.17%\n","mAP@0.4 =0.79%\n","mAP@0.5 =0.19%\n","average MAP: 1.37\n","epoch: 1 | step: 2560 | loss1: 0.4895 | loss2: 2.2704 | loss3: 0.0945\n","AUC1:  0.794575799736916  AP1:  0.18613478740241668\n","AUC2:  0.7943967722021964  AP2: 0.1909005789987792\n","mAP@0.1 =1.65%\n","mAP@0.2 =1.21%\n","mAP@0.3 =0.91%\n","mAP@0.4 =0.39%\n","mAP@0.5 =0.16%\n","average MAP: 0.86\n","epoch: 1 | step: 3840 | loss1: 0.4368 | loss2: 2.0856 | loss3: 0.0780\n","AUC1:  0.8033252779522663  AP1:  0.19199043662060675\n","AUC2:  0.7987365114071406  AP2: 0.17942550302258417\n","mAP@0.1 =3.24%\n","mAP@0.2 =2.65%\n","mAP@0.3 =2.11%\n","mAP@0.4 =0.38%\n","mAP@0.5 =0.21%\n","average MAP: 1.72\n","epoch: 1 | step: 5120 | loss1: 0.3981 | loss2: 1.9399 | loss3: 0.0663\n","AUC1:  0.8055854052525966  AP1:  0.19861231576427862\n","AUC2:  0.8078892104246138  AP2: 0.19331347642961827\n","mAP@0.1 =4.56%\n","mAP@0.2 =3.80%\n","mAP@0.3 =2.60%\n","mAP@0.4 =2.09%\n","mAP@0.5 =1.12%\n","average MAP: 2.83\n","epoch: 1 | step: 6400 | loss1: 0.3734 | loss2: 1.8370 | loss3: 0.0603\n","AUC1:  0.8105541663992748  AP1:  0.19946853660433994\n","AUC2:  0.8112162979049908  AP2: 0.19218970006938169\n","mAP@0.1 =6.65%\n","mAP@0.2 =4.53%\n","mAP@0.3 =3.48%\n","mAP@0.4 =2.88%\n","mAP@0.5 =0.82%\n","average MAP: 3.67\n","epoch: 1 | step: 7680 | loss1: 0.3556 | loss2: 1.7657 | loss3: 0.0619\n","AUC1:  0.8133348766620598  AP1:  0.2039656534903437\n","AUC2:  0.8192000046010167  AP2: 0.20540891725512947\n","mAP@0.1 =5.73%\n","mAP@0.2 =3.68%\n","mAP@0.3 =1.81%\n","mAP@0.4 =1.05%\n","mAP@0.5 =0.26%\n","average MAP: 2.51\n","epoch: 1 | step: 8960 | loss1: 0.3394 | loss2: 1.6962 | loss3: 0.0538\n","AUC1:  0.8178198226670605  AP1:  0.2081750206445535\n","AUC2:  0.8219007228044477  AP2: 0.20991601367658405\n","mAP@0.1 =6.52%\n","mAP@0.2 =4.38%\n","mAP@0.3 =2.56%\n","mAP@0.4 =2.02%\n","mAP@0.5 =0.53%\n","average MAP: 3.20\n","epoch: 1 | step: 10240 | loss1: 0.3257 | loss2: 1.6391 | loss3: 0.0515\n","AUC1:  0.8221554956697656  AP1:  0.21252631206807956\n","AUC2:  0.8355719098019537  AP2: 0.218684876132099\n","mAP@0.1 =6.79%\n","mAP@0.2 =4.46%\n","mAP@0.3 =3.32%\n","mAP@0.4 =2.75%\n","mAP@0.5 =1.07%\n","average MAP: 3.68\n","epoch: 1 | step: 11520 | loss1: 0.3143 | loss2: 1.5903 | loss3: 0.0556\n","AUC1:  0.82525424379659  AP1:  0.211752247044043\n","AUC2:  0.8357286678895713  AP2: 0.2168969700003712\n","mAP@0.1 =7.40%\n","mAP@0.2 =4.03%\n","mAP@0.3 =1.40%\n","mAP@0.4 =0.60%\n","mAP@0.5 =0.44%\n","average MAP: 2.78\n"]}]}]}