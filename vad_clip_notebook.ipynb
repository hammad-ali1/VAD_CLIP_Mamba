{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aNpPTDBgNr1nK63ztQXatuHhXMJLsGAa","authorship_tag":"ABX9TyMDbX28H18zw2HQ7egih6NT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hammad-ali1/vad_clip_colab_notebook/blob/main/vad_clip_notebook.ipynb)\n"],"metadata":{"id":"hjJEkZzImzOk"}},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxKtd3_hImCC","executionInfo":{"status":"ok","timestamp":1752388826345,"user_tz":-300,"elapsed":29289,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"96898f6b-437b-48f1-860a-fe2e330a006a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#@title Mount Drive\n","import os\n","if os.path.ismount('/content/drive'):\n","    print(\"Google Drive is already mounted.\")\n","else:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DfMHfDVZPB8","executionInfo":{"status":"ok","timestamp":1752388610503,"user_tz":-300,"elapsed":5717,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"69816045-be2a-407c-9406-bfde538a89f9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n"]}]},{"cell_type":"code","source":["#@title Pull Latest Repo\n","from pathlib import Path\n","from google.colab import userdata\n","\n","NOTEBOOK_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/vad_clip\")\n","NOTEBOOK_PATH = NOTEBOOK_DIR / \"vad_clip_notebook.ipynb\"\n","\n","gh_token = userdata.get('vad_clip_gh_token')\n","\n","REPO_NAME = \"vad_clip_colab_notebook\"\n","REPO_PATH = Path(\"/content\") / REPO_NAME\n","\n","if gh_token:\n","  GITHUB_URL = f\"https://{gh_token}@github.com/hammad-ali1/{REPO_NAME}.git\"\n","else:\n","  GITHUB_URL = f'https://github.com/hammad-ali1/{REPO_NAME}'\n","\n","!rm -rf {REPO_PATH}\n","!git clone {GITHUB_URL}\n","\n","!cp -r {REPO_PATH}/* /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3JvbCrKXHfc","executionInfo":{"status":"ok","timestamp":1752388616536,"user_tz":-300,"elapsed":4366,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"86ff434c-5332-4084-ad2a-c9035b5ae4f2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vad_clip_colab_notebook'...\n","remote: Enumerating objects: 61, done.\u001b[K\n","remote: Counting objects: 100% (61/61), done.\u001b[K\n","remote: Compressing objects: 100% (50/50), done.\u001b[K\n","remote: Total 61 (delta 13), reused 48 (delta 9), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (61/61), 1.63 MiB | 1.31 MiB/s, done.\n","Resolving deltas: 100% (13/13), done.\n"]}]},{"cell_type":"code","source":["#@title Push Notebook to Github\n","def push_notebook_to_gh():\n","  commit_msg = input(\"Commit message: \")\n","  !cp \"{NOTEBOOK_PATH}\" {REPO_PATH}\n","  !cd {REPO_PATH} && git config user.name \"Hammad Ali\"\n","  !cd {REPO_PATH} && git config user.email \"hammad.a22002@gmail.com\"\n","\n","  # Add, commit and push\n","  !cd {REPO_PATH} && git add .\n","  !cd {REPO_PATH} && git commit -m \"{commit_msg}\"\n","  !cd {REPO_PATH} && git push"],"metadata":{"id":"CAuhT-UbJJ6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Import Modules\n","from collections import OrderedDict\n","import os\n","from pathlib import Path\n","\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import average_precision_score, roc_auc_score\n","\n","from clip import clip\n","from utils.dataset import UCFDataset\n","from utils.layers import GraphConvolution, DistanceAdj\n","from utils.tools import get_batch_mask, get_prompt_text\n","from utils.ucf_detectionMAP import getDetectionMAP as dmAP"],"metadata":{"id":"F_pOyYg7Ogdd","executionInfo":{"status":"ok","timestamp":1752388633851,"user_tz":-300,"elapsed":10200,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class Args:\n","    seed = 234\n","\n","    embed_dim = 512\n","    visual_length = 256\n","    visual_width = 512\n","    visual_head = 1\n","    visual_layers = 2\n","    attn_window = 8\n","    prompt_prefix = 10\n","    prompt_postfix = 10\n","    classes_num = 14\n","\n","    max_epoch = 10\n","    model_path = NOTEBOOK_DIR / \"model\" / \"model_cur.pth\"\n","    use_checkpoint = False\n","    checkpoint_path = NOTEBOOK_DIR / \"model\" / \"checkpoint_v2.pth\"\n","    batch_size = 64\n","    train_list = 'list/ucf_CLIP_rgb.csv'\n","    test_list = 'list/ucf_CLIP_rgbtest.csv'\n","    gt_path = 'list/gt_ucf.npy'\n","    gt_segment_path = 'list/gt_segment_ucf.npy'\n","    gt_label_path = 'list/gt_label_ucf.npy'\n","\n","    lr = 2e-5\n","    scheduler_rate = 0.1\n","    scheduler_milestones = [4, 8]\n"],"metadata":{"id":"0VPhYpQPbBSN","executionInfo":{"status":"ok","timestamp":1752388633903,"user_tz":-300,"elapsed":21,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#@title Model\n","class LayerNorm(nn.LayerNorm):\n","\n","    def forward(self, x: torch.Tensor):\n","        orig_type = x.dtype\n","        ret = super().forward(x.type(torch.float32))\n","        return ret.type(orig_type)\n","\n","\n","class QuickGELU(nn.Module):\n","    def forward(self, x: torch.Tensor):\n","        return x * torch.sigmoid(1.702 * x)\n","\n","\n","class ResidualAttentionBlock(nn.Module):\n","    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n","        super().__init__()\n","\n","        self.attn = nn.MultiheadAttention(d_model, n_head)\n","        self.ln_1 = LayerNorm(d_model)\n","        self.mlp = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n","        ]))\n","        self.ln_2 = LayerNorm(d_model)\n","        self.attn_mask = attn_mask\n","\n","    def attention(self, x: torch.Tensor, padding_mask: torch.Tensor):\n","        padding_mask = padding_mask.to(dtype=bool, device=x.device) if padding_mask is not None else None\n","        self.attn_mask = self.attn_mask.to(device=x.device) if self.attn_mask is not None else None\n","        return self.attn(x, x, x, need_weights=False, key_padding_mask=padding_mask, attn_mask=self.attn_mask)[0]\n","\n","    def forward(self, x):\n","        x, padding_mask = x\n","        x = x + self.attention(self.ln_1(x), padding_mask)\n","        x = x + self.mlp(self.ln_2(x))\n","        return (x, padding_mask)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n","        super().__init__()\n","        self.width = width\n","        self.layers = layers\n","        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n","\n","    def forward(self, x: torch.Tensor):\n","        return self.resblocks(x)\n","\n","\n","class CLIPVAD(nn.Module):\n","    def __init__(self,\n","                 num_class: int,\n","                 embed_dim: int,\n","                 visual_length: int,\n","                 visual_width: int,\n","                 visual_head: int,\n","                 visual_layers: int,\n","                 attn_window: int,\n","                 prompt_prefix: int,\n","                 prompt_postfix: int,\n","                 device):\n","        super().__init__()\n","\n","        self.num_class = num_class\n","        self.visual_length = visual_length\n","        self.visual_width = visual_width\n","        self.embed_dim = embed_dim\n","        self.attn_window = attn_window\n","        self.prompt_prefix = prompt_prefix\n","        self.prompt_postfix = prompt_postfix\n","        self.device = device\n","\n","        self.temporal = Transformer(\n","            width=visual_width,\n","            layers=visual_layers,\n","            heads=visual_head,\n","            attn_mask=self.build_attention_mask(self.attn_window)\n","        )\n","\n","        width = int(visual_width / 2)\n","        self.gc1 = GraphConvolution(visual_width, width, residual=True)\n","        self.gc2 = GraphConvolution(width, width, residual=True)\n","        self.gc3 = GraphConvolution(visual_width, width, residual=True)\n","        self.gc4 = GraphConvolution(width, width, residual=True)\n","        self.disAdj = DistanceAdj()\n","        self.linear = nn.Linear(visual_width, visual_width)\n","        self.gelu = QuickGELU()\n","\n","        self.mlp1 = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n","        ]))\n","        self.mlp2 = nn.Sequential(OrderedDict([\n","            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n","            (\"gelu\", QuickGELU()),\n","            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n","        ]))\n","        self.classifier = nn.Linear(visual_width, 1)\n","\n","        self.clipmodel, _ = clip.load(\"ViT-B/16\", device)\n","        for clip_param in self.clipmodel.parameters():\n","            clip_param.requires_grad = False\n","\n","        self.frame_position_embeddings = nn.Embedding(visual_length, visual_width)\n","        self.text_prompt_embeddings = nn.Embedding(77, self.embed_dim)\n","\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        nn.init.normal_(self.text_prompt_embeddings.weight, std=0.01)\n","        nn.init.normal_(self.frame_position_embeddings.weight, std=0.01)\n","\n","    def build_attention_mask(self, attn_window):\n","        # lazily create causal attention mask, with full attention between the vision tokens\n","        # pytorch uses additive attention mask; fill with -inf\n","        mask = torch.empty(self.visual_length, self.visual_length)\n","        mask.fill_(float('-inf'))\n","        for i in range(int(self.visual_length / attn_window)):\n","            if (i + 1) * attn_window < self.visual_length:\n","                mask[i * attn_window: (i + 1) * attn_window, i * attn_window: (i + 1) * attn_window] = 0\n","            else:\n","                mask[i * attn_window: self.visual_length, i * attn_window: self.visual_length] = 0\n","\n","        return mask\n","\n","    def adj4(self, x, seq_len):\n","        soft = nn.Softmax(1)\n","        x2 = x.matmul(x.permute(0, 2, 1)) # B*T*T\n","        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n","        x_norm_x = x_norm.matmul(x_norm.permute(0, 2, 1))\n","        x2 = x2/(x_norm_x+1e-20)\n","        output = torch.zeros_like(x2)\n","        if seq_len is None:\n","            for i in range(x.shape[0]):\n","                tmp = x2[i]\n","                adj2 = tmp\n","                adj2 = F.threshold(adj2, 0.7, 0)\n","                adj2 = soft(adj2)\n","                output[i] = adj2\n","        else:\n","            for i in range(len(seq_len)):\n","                tmp = x2[i, :seq_len[i], :seq_len[i]]\n","                adj2 = tmp\n","                adj2 = F.threshold(adj2, 0.7, 0)\n","                adj2 = soft(adj2)\n","                output[i, :seq_len[i], :seq_len[i]] = adj2\n","\n","        return output\n","\n","    def encode_video(self, images, padding_mask, lengths):\n","        images = images.to(torch.float)\n","        position_ids = torch.arange(self.visual_length, device=self.device)\n","        position_ids = position_ids.unsqueeze(0).expand(images.shape[0], -1)\n","        frame_position_embeddings = self.frame_position_embeddings(position_ids)\n","        frame_position_embeddings = frame_position_embeddings.permute(1, 0, 2)\n","        images = images.permute(1, 0, 2) + frame_position_embeddings\n","\n","        x, _ = self.temporal((images, None))\n","        x = x.permute(1, 0, 2)\n","\n","        adj = self.adj4(x, lengths)\n","        disadj = self.disAdj(x.shape[0], x.shape[1])\n","        x1_h = self.gelu(self.gc1(x, adj))\n","        x2_h = self.gelu(self.gc3(x, disadj))\n","\n","        x1 = self.gelu(self.gc2(x1_h, adj))\n","        x2 = self.gelu(self.gc4(x2_h, disadj))\n","\n","        x = torch.cat((x1, x2), 2)\n","        x = self.linear(x)\n","\n","        return x\n","\n","    def encode_textprompt(self, text):\n","        word_tokens = clip.tokenize(text).to(self.device)\n","        word_embedding = self.clipmodel.encode_token(word_tokens)\n","        text_embeddings = self.text_prompt_embeddings(torch.arange(77).to(self.device)).unsqueeze(0).repeat([len(text), 1, 1])\n","        text_tokens = torch.zeros(len(text), 77).to(self.device)\n","\n","        for i in range(len(text)):\n","            ind = torch.argmax(word_tokens[i], -1)\n","            text_embeddings[i, 0] = word_embedding[i, 0]\n","            text_embeddings[i, self.prompt_prefix + 1: self.prompt_prefix + ind] = word_embedding[i, 1: ind]\n","            text_embeddings[i, self.prompt_prefix + ind + self.prompt_postfix] = word_embedding[i, ind]\n","            text_tokens[i, self.prompt_prefix + ind + self.prompt_postfix] = word_tokens[i, ind]\n","\n","        text_features = self.clipmodel.encode_text(text_embeddings, text_tokens)\n","\n","        return text_features\n","\n","    def forward(self, visual, padding_mask, text, lengths):\n","        visual_features = self.encode_video(visual, padding_mask, lengths)\n","        logits1 = self.classifier(visual_features + self.mlp2(visual_features))\n","\n","        text_features_ori = self.encode_textprompt(text)\n","\n","        text_features = text_features_ori\n","        logits_attn = logits1.permute(0, 2, 1)\n","        visual_attn = logits_attn @ visual_features\n","        visual_attn = visual_attn / visual_attn.norm(dim=-1, keepdim=True)\n","        visual_attn = visual_attn.expand(visual_attn.shape[0], text_features_ori.shape[0], visual_attn.shape[2])\n","        text_features = text_features_ori.unsqueeze(0)\n","        text_features = text_features.expand(visual_attn.shape[0], text_features.shape[1], text_features.shape[2])\n","        text_features = text_features + visual_attn\n","        text_features = text_features + self.mlp1(text_features)\n","\n","        visual_features_norm = visual_features / visual_features.norm(dim=-1, keepdim=True)\n","        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n","        text_features_norm = text_features_norm.permute(0, 2, 1)\n","        logits2 = visual_features_norm @ text_features_norm.type(visual_features_norm.dtype) / 0.07\n","\n","        return text_features_ori, logits1, logits2\n"],"metadata":{"id":"2ODZ8AzzN3_M","executionInfo":{"status":"ok","timestamp":1752388633953,"user_tz":-300,"elapsed":25,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title Test Function\n","def test(model, testdataloader, maxlen, prompt_text, gt, gtsegments, gtlabels, device):\n","\n","    model.to(device)\n","    model.eval()\n","\n","    element_logits2_stack = []\n","\n","    with torch.no_grad():\n","        for i, item in enumerate(testdataloader):\n","            visual = item[0].squeeze(0)\n","            length = item[2]\n","\n","            length = int(length)\n","            len_cur = length\n","            if len_cur < maxlen:\n","                visual = visual.unsqueeze(0)\n","\n","            visual = visual.to(device)\n","\n","            lengths = torch.zeros(int(length / maxlen) + 1)\n","            for j in range(int(length / maxlen) + 1):\n","                if j == 0 and length < maxlen:\n","                    lengths[j] = length\n","                elif j == 0 and length > maxlen:\n","                    lengths[j] = maxlen\n","                    length -= maxlen\n","                elif length > maxlen:\n","                    lengths[j] = maxlen\n","                    length -= maxlen\n","                else:\n","                    lengths[j] = length\n","            lengths = lengths.to(int)\n","            padding_mask = get_batch_mask(lengths, maxlen).to(device)\n","            _, logits1, logits2 = model(visual, padding_mask, prompt_text, lengths)\n","            logits1 = logits1.reshape(logits1.shape[0] * logits1.shape[1], logits1.shape[2])\n","            logits2 = logits2.reshape(logits2.shape[0] * logits2.shape[1], logits2.shape[2])\n","            prob2 = (1 - logits2[0:len_cur].softmax(dim=-1)[:, 0].squeeze(-1))\n","            prob1 = torch.sigmoid(logits1[0:len_cur].squeeze(-1))\n","\n","            if i == 0:\n","                ap1 = prob1\n","                ap2 = prob2\n","                #ap3 = prob3\n","            else:\n","                ap1 = torch.cat([ap1, prob1], dim=0)\n","                ap2 = torch.cat([ap2, prob2], dim=0)\n","\n","            element_logits2 = logits2[0:len_cur].softmax(dim=-1).detach().cpu().numpy()\n","            element_logits2 = np.repeat(element_logits2, 16, 0)\n","            element_logits2_stack.append(element_logits2)\n","\n","    ap1 = ap1.cpu().numpy()\n","    ap2 = ap2.cpu().numpy()\n","    ap1 = ap1.tolist()\n","    ap2 = ap2.tolist()\n","\n","    ROC1 = roc_auc_score(gt, np.repeat(ap1, 16))\n","    AP1 = average_precision_score(gt, np.repeat(ap1, 16))\n","    ROC2 = roc_auc_score(gt, np.repeat(ap2, 16))\n","    AP2 = average_precision_score(gt, np.repeat(ap2, 16))\n","\n","    print(\"AUC1: \", ROC1, \" AP1: \", AP1)\n","    print(\"AUC2: \", ROC2, \" AP2:\", AP2)\n","\n","    dmap, iou = dmAP(element_logits2_stack, gtsegments, gtlabels, excludeNormal=False)\n","    averageMAP = 0\n","    for i in range(5):\n","        print('mAP@{0:.1f} ={1:.2f}%'.format(iou[i], dmap[i]))\n","        averageMAP += dmap[i]\n","    averageMAP = averageMAP/(i+1)\n","    print('average MAP: {:.2f}'.format(averageMAP))\n","\n","    return ROC1, AP1\n","\n","\n","def run_test():\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args = Args()\n","\n","    label_map = dict({'Normal': 'Normal', 'Abuse': 'Abuse', 'Arrest': 'Arrest', 'Arson': 'Arson', 'Assault': 'Assault', 'Burglary': 'Burglary', 'Explosion': 'Explosion', 'Fighting': 'Fighting', 'RoadAccidents': 'RoadAccidents', 'Robbery': 'Robbery', 'Shooting': 'Shooting', 'Shoplifting': 'Shoplifting', 'Stealing': 'Stealing', 'Vandalism': 'Vandalism'})\n","\n","    testdataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n","    testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)\n","\n","    prompt_text = get_prompt_text(label_map)\n","    gt = np.load(args.gt_path)\n","    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n","    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n","\n","    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n","    model_param = torch.load(args.model_path)\n","    model.load_state_dict(model_param)\n","\n","    test(model, testdataloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)"],"metadata":{"id":"8LrMWNfSZ-RX","cellView":"form","executionInfo":{"status":"ok","timestamp":1752388638316,"user_tz":-300,"elapsed":67,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#@title Train Function\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import MultiStepLR\n","import numpy as np\n","import random\n","import os\n","\n","from utils.dataset import UCFDataset\n","from utils.tools import get_prompt_text, get_batch_label\n","\n","def CLASM(logits, labels, lengths, device):\n","    instance_logits = torch.zeros(0).to(device)\n","    labels = labels / torch.sum(labels, dim=1, keepdim=True)\n","    labels = labels.to(device)\n","\n","    for i in range(logits.shape[0]):\n","        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True, dim=0)\n","        instance_logits = torch.cat([instance_logits, torch.mean(tmp, 0, keepdim=True)], dim=0)\n","\n","    milloss = -torch.mean(torch.sum(labels * F.log_softmax(instance_logits, dim=1), dim=1), dim=0)\n","    return milloss\n","\n","def CLAS2(logits, labels, lengths, device):\n","    instance_logits = torch.zeros(0).to(device)\n","    labels = 1 - labels[:, 0].reshape(labels.shape[0])\n","    labels = labels.to(device)\n","    logits = torch.sigmoid(logits).reshape(logits.shape[0], logits.shape[1])\n","\n","    for i in range(logits.shape[0]):\n","        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True)\n","        tmp = torch.mean(tmp).view(1)\n","        instance_logits = torch.cat([instance_logits, tmp], dim=0)\n","\n","    clsloss = F.binary_cross_entropy(instance_logits, labels)\n","    return clsloss\n","\n","def train(model, normal_loader, anomaly_loader, testloader, args, label_map, device):\n","    model.to(device)\n","    gt = np.load(args.gt_path)\n","    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n","    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n","    scheduler = MultiStepLR(optimizer, args.scheduler_milestones, args.scheduler_rate)\n","    prompt_text = get_prompt_text(label_map)\n","    ap_best = 0\n","    epoch = 0\n","\n","    if args.use_checkpoint == True:\n","        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        ap_best = checkpoint['ap']\n","        print(\"checkpoint info:\")\n","        print(\"epoch:\", epoch+1, \" ap:\", ap_best)\n","\n","    for e in range(args.max_epoch):\n","        model.train()\n","        loss_total1 = 0\n","        loss_total2 = 0\n","        normal_iter = iter(normal_loader)\n","        anomaly_iter = iter(anomaly_loader)\n","        for i in range(min(len(normal_loader), len(anomaly_loader))):\n","            step = 0\n","            normal_features, normal_label, normal_lengths = next(normal_iter)\n","            anomaly_features, anomaly_label, anomaly_lengths = next(anomaly_iter)\n","\n","            visual_features = torch.cat([normal_features, anomaly_features], dim=0).to(device)\n","            text_labels = list(normal_label) + list(anomaly_label)\n","            feat_lengths = torch.cat([normal_lengths, anomaly_lengths], dim=0).to(device)\n","            text_labels = get_batch_label(text_labels, prompt_text, label_map).to(device)\n","\n","            text_features, logits1, logits2 = model(visual_features, None, prompt_text, feat_lengths)\n","            #loss1\n","            loss1 = CLAS2(logits1, text_labels, feat_lengths, device)\n","            loss_total1 += loss1.item()\n","            #loss2\n","            loss2 = CLASM(logits2, text_labels, feat_lengths, device)\n","            loss_total2 += loss2.item()\n","            #loss3\n","            loss3 = torch.zeros(1).to(device)\n","            text_feature_normal = text_features[0] / text_features[0].norm(dim=-1, keepdim=True)\n","            for j in range(1, text_features.shape[0]):\n","                text_feature_abr = text_features[j] / text_features[j].norm(dim=-1, keepdim=True)\n","                loss3 += torch.abs(text_feature_normal @ text_feature_abr)\n","            loss3 = loss3 / 13 * 1e-1\n","\n","            loss = loss1 + loss2 + loss3\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            step += i * normal_loader.batch_size * 2\n","            if step % 1280 == 0 and step != 0:\n","                print('epoch: ', e+1, '| step: ', step, '| loss1: ', loss_total1 / (i+1), '| loss2: ', loss_total2 / (i+1), '| loss3: ', loss3.item())\n","                AUC, AP = test(model, testloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)\n","                AP = AUC\n","\n","                if AP > ap_best:\n","                    ap_best = AP\n","                    checkpoint = {\n","                        'epoch': e,\n","                        'model_state_dict': model.state_dict(),\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        'ap': ap_best}\n","                    torch.save(checkpoint, args.checkpoint_path)\n","\n","        scheduler.step()\n","\n","        checkpoint_dir = os.path.dirname(args.checkpoint_path)\n","        save_path = os.path.join(checkpoint_dir, 'model_cur.pth')\n","        torch.save(model.state_dict(), save_path)\n","        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n","    torch.save(checkpoint['model_state_dict'], args.model_path)\n","\n","def setup_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    #torch.backends.cudnn.deterministic = True\n","\n","def run_train():\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args = Args()\n","    setup_seed(args.seed)\n","\n","    label_map = dict({'Normal': 'normal', 'Abuse': 'abuse', 'Arrest': 'arrest', 'Arson': 'arson', 'Assault': 'assault', 'Burglary': 'burglary', 'Explosion': 'explosion', 'Fighting': 'fighting', 'RoadAccidents': 'roadAccidents', 'Robbery': 'robbery', 'Shooting': 'shooting', 'Shoplifting': 'shoplifting', 'Stealing': 'stealing', 'Vandalism': 'vandalism'})\n","\n","    normal_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, True)\n","    normal_loader = DataLoader(normal_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n","    anomaly_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, False)\n","    anomaly_loader = DataLoader(anomaly_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n","\n","    test_dataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n","\n","    train(model, normal_loader, anomaly_loader, test_loader, args, label_map, device)\n"],"metadata":{"id":"eyaLSBFMpdWk","executionInfo":{"status":"ok","timestamp":1752388693578,"user_tz":-300,"elapsed":26,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["run_train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"bBtt-FAfgY2A","executionInfo":{"status":"error","timestamp":1752393301048,"user_tz":-300,"elapsed":2160708,"user":{"displayName":"MAHA KHALID","userId":"05868234410741760598"}},"outputId":"c3d6fe0f-4454-4e04-d09f-890062d2a860"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:  1 | step:  1280 | loss1:  0.6959322853521868 | loss2:  2.511510892347856 | loss3:  0.09681253135204315\n","AUC1:  0.7785882321532747  AP1:  0.21523007161414348\n","AUC2:  0.4170853564659542  AP2: 0.061716732403916855\n","mAP@0.1 =1.57%\n","mAP@0.2 =1.32%\n","mAP@0.3 =1.05%\n","mAP@0.4 =0.87%\n","mAP@0.5 =0.83%\n","average MAP: 1.13\n","epoch:  1 | step:  2560 | loss1:  0.5284017799865632 | loss2:  2.3840801261720204 | loss3:  0.09481126815080643\n","AUC1:  0.8142300510340676  AP1:  0.24240418299650066\n","AUC2:  0.6130951734052458  AP2: 0.11227340604948316\n","mAP@0.1 =2.50%\n","mAP@0.2 =2.22%\n","mAP@0.3 =1.84%\n","mAP@0.4 =1.68%\n","mAP@0.5 =1.55%\n","average MAP: 1.96\n","epoch:  1 | step:  3840 | loss1:  0.45333201942905305 | loss2:  2.250479336707823 | loss3:  0.09130697697401047\n","AUC1:  0.8337690707174039  AP1:  0.25450257828744827\n","AUC2:  0.7832071193808582  AP2: 0.20219909358460159\n","mAP@0.1 =3.13%\n","mAP@0.2 =2.23%\n","mAP@0.3 =1.96%\n","mAP@0.4 =1.49%\n","mAP@0.5 =1.15%\n","average MAP: 1.99\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-14-1009202271.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-10-749186701.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIPVAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_postfix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-10-749186701.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, normal_loader, anomaly_loader, testloader, args, label_map, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mnormal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0manomaly_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mvisual_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mclip_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mclip_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m  \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data left in file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}