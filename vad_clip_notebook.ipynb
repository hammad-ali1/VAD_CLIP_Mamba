{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hammad-ali1/vad_clip_colab_notebook/blob/main/vad_clip_notebook.ipynb)\n"
      ],
      "metadata": {
        "id": "hjJEkZzImzOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install ftfy\n",
        "!pip install comet_ml\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "6DfMHfDVZPB8",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pull Latest Repo\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "  gh_token = userdata.get('vad_clip_gh_token')\n",
        "except Exception:\n",
        "  gh_token = None\n",
        "\n",
        "REPO_NAME = \"vad_clip_colab_notebook\"\n",
        "REPO_PATH = Path(\"/content\") / REPO_NAME\n",
        "\n",
        "if gh_token:\n",
        "  GITHUB_URL = f\"https://{gh_token}@github.com/hammad-ali1/{REPO_NAME}.git\"\n",
        "else:\n",
        "  GITHUB_URL = f'https://github.com/hammad-ali1/{REPO_NAME}'\n",
        "\n",
        "!rm -rf {REPO_PATH}\n",
        "!git clone {GITHUB_URL}\n",
        "\n",
        "!cp -r {REPO_PATH}/* /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3JvbCrKXHfc",
        "outputId": "f5363a9c-c0c1-4661-a247-14134037c9ba",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vad_clip_colab_notebook'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 82 (delta 26), reused 52 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (82/82), 1.68 MiB | 8.31 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Modules\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "import comet_ml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "from clip import clip\n",
        "from utils.dataset import UCFDataset\n",
        "from utils.layers import GraphConvolution, DistanceAdj\n",
        "from utils.tools import get_batch_mask, get_prompt_text, get_batch_label\n",
        "from utils.ucf_detectionMAP import getDetectionMAP as dmAP"
      ],
      "metadata": {
        "id": "F_pOyYg7Ogdd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_DATASET_DIR = Path('/content/drive/MyDrive/UCFClipFeatures')\n",
        "\n",
        "if os.path.ismount('/content/drive') and DRIVE_DATASET_DIR.exists():\n",
        "  DATASET_DIR = DRIVE_DATASET_DIR\n",
        "else:\n",
        "  !gdown 1DOkXHObwEGqmU3c9-J1gCIyVXiTMFfC5\n",
        "  !unzip /content/UCFClipFeatures.zip -d /content\n",
        "  !rm /content/UCFClipFeatures.zip\n",
        "  DATASET_DIR = Path('/content/UCFClipFeatures')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "aXPCB6SdOqzk",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    seed = 234\n",
        "\n",
        "    embed_dim = 512\n",
        "    visual_length = 256\n",
        "    visual_width = 512\n",
        "    visual_head = 1\n",
        "    visual_layers = 2\n",
        "    attn_window = 8\n",
        "    prompt_prefix = 10\n",
        "    prompt_postfix = 10\n",
        "    classes_num = 14\n",
        "\n",
        "    max_epoch = 10\n",
        "    model_path = \"model_cur.pth\"\n",
        "    use_checkpoint = False\n",
        "    checkpoint_path = \"checkpoint.pth\"\n",
        "    batch_size = 64\n",
        "    train_list = 'list/ucf_CLIP_rgb.csv'\n",
        "    test_list = 'list/ucf_CLIP_rgbtest.csv'\n",
        "    gt_path = 'list/gt_ucf.npy'\n",
        "    gt_segment_path = 'list/gt_segment_ucf.npy'\n",
        "    gt_label_path = 'list/gt_label_ucf.npy'\n",
        "\n",
        "    lr = 2e-5\n",
        "    scheduler_rate = 0.1\n",
        "    scheduler_milestones = [4, 8]\n"
      ],
      "metadata": {
        "id": "0VPhYpQPbBSN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_lists(file_path, limit_list=None):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract video ID (removing part suffix)\n",
        "    df['video_id'] = df['path'].apply(lambda x: x.rsplit('__', 1)[0])\n",
        "\n",
        "    # Keep only complete videos (with all 10 parts)\n",
        "    grouped = df.groupby('video_id')\n",
        "    complete_videos = grouped.filter(lambda x: len(x) == 10)\n",
        "\n",
        "    if limit_list is not None:\n",
        "        total_videos = limit_list // 10\n",
        "\n",
        "        # Create video-level metadata\n",
        "        video_meta = (\n",
        "            complete_videos.groupby('video_id')\n",
        "            .first()\n",
        "            .reset_index()[['video_id', 'label']]\n",
        "        )\n",
        "\n",
        "        # Separate Normal and Other labels\n",
        "        normal_videos = video_meta[video_meta['label'] == 'Normal']\n",
        "        other_videos = video_meta[video_meta['label'] != 'Normal']\n",
        "        other_labels = other_videos['label'].unique()\n",
        "\n",
        "        num_normal_videos = total_videos // 2\n",
        "        num_other_videos = total_videos - num_normal_videos\n",
        "        per_other_label = max(1, num_other_videos // len(other_labels))\n",
        "\n",
        "        # Sample Normal videos\n",
        "        selected_videos = normal_videos.sample(\n",
        "            n=min(num_normal_videos, len(normal_videos)), random_state=42\n",
        "        )\n",
        "\n",
        "        # Sample from other labels\n",
        "        for label in other_labels:\n",
        "            vids = other_videos[other_videos['label'] == label]\n",
        "            sampled = vids.sample(n=min(per_other_label, len(vids)), random_state=42)\n",
        "            selected_videos = pd.concat([selected_videos, sampled], ignore_index=True)\n",
        "\n",
        "        # Get all 10 parts of selected videos\n",
        "        final_df = complete_videos[complete_videos['video_id'].isin(selected_videos['video_id'])]\n",
        "\n",
        "        # Sort by video and clip number\n",
        "        final_df['part'] = final_df['path'].apply(lambda x: int(Path(x).stem.split('__')[-1]))\n",
        "        final_df = final_df.sort_values(by=['label', 'video_id', 'part']).drop(columns='part')\n",
        "    else:\n",
        "        final_df = df\n",
        "\n",
        "    # Fix paths relative to DATASET_DIR\n",
        "    old_root = Path('/content/drive/MyDrive/UCFClipFeatures')\n",
        "    final_df['path'] = final_df['path'].apply(lambda p: str(DATASET_DIR / Path(p).relative_to(old_root)))\n",
        "\n",
        "    # Save\n",
        "    final_df.drop(columns='video_id').to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "57aXEliJT-Q0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_lists(Args.test_list)\n",
        "update_lists(Args.train_list, limit_list=3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc_UEDSqVmDB",
        "outputId": "c3afd644-7bd7-409b-84f9-0c0436c69698"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-626387157.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['part'] = final_df['path'].apply(lambda x: int(Path(x).stem.split('__')[-1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "class LayerNorm(nn.LayerNorm):\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        orig_type = x.dtype\n",
        "        ret = super().forward(x.type(torch.float32))\n",
        "        return ret.type(orig_type)\n",
        "\n",
        "\n",
        "class QuickGELU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
        "        ]))\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "    def attention(self, x: torch.Tensor, padding_mask: torch.Tensor):\n",
        "        padding_mask = padding_mask.to(dtype=bool, device=x.device) if padding_mask is not None else None\n",
        "        self.attn_mask = self.attn_mask.to(device=x.device) if self.attn_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, key_padding_mask=padding_mask, attn_mask=self.attn_mask)[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, padding_mask = x\n",
        "        x = x + self.attention(self.ln_1(x), padding_mask)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return (x, padding_mask)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.layers = layers\n",
        "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.resblocks(x)\n",
        "\n",
        "\n",
        "class CLIPVAD(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_class: int,\n",
        "                 embed_dim: int,\n",
        "                 visual_length: int,\n",
        "                 visual_width: int,\n",
        "                 visual_head: int,\n",
        "                 visual_layers: int,\n",
        "                 attn_window: int,\n",
        "                 prompt_prefix: int,\n",
        "                 prompt_postfix: int,\n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_class = num_class\n",
        "        self.visual_length = visual_length\n",
        "        self.visual_width = visual_width\n",
        "        self.embed_dim = embed_dim\n",
        "        self.attn_window = attn_window\n",
        "        self.prompt_prefix = prompt_prefix\n",
        "        self.prompt_postfix = prompt_postfix\n",
        "        self.device = device\n",
        "\n",
        "        self.temporal = Transformer(\n",
        "            width=visual_width,\n",
        "            layers=visual_layers,\n",
        "            heads=visual_head,\n",
        "            attn_mask=self.build_attention_mask(self.attn_window)\n",
        "        )\n",
        "\n",
        "        width = int(visual_width / 2)\n",
        "        self.gc1 = GraphConvolution(visual_width, width, residual=True)\n",
        "        self.gc2 = GraphConvolution(width, width, residual=True)\n",
        "        self.gc3 = GraphConvolution(visual_width, width, residual=True)\n",
        "        self.gc4 = GraphConvolution(width, width, residual=True)\n",
        "        self.disAdj = DistanceAdj()\n",
        "        self.linear = nn.Linear(visual_width, visual_width)\n",
        "        self.gelu = QuickGELU()\n",
        "\n",
        "        self.mlp1 = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n",
        "        ]))\n",
        "        self.mlp2 = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(visual_width, visual_width * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(visual_width * 4, visual_width))\n",
        "        ]))\n",
        "        self.classifier = nn.Linear(visual_width, 1)\n",
        "\n",
        "        self.clipmodel, _ = clip.load(\"ViT-B/16\", device)\n",
        "        for clip_param in self.clipmodel.parameters():\n",
        "            clip_param.requires_grad = False\n",
        "\n",
        "        self.frame_position_embeddings = nn.Embedding(visual_length, visual_width)\n",
        "        self.text_prompt_embeddings = nn.Embedding(77, self.embed_dim)\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.text_prompt_embeddings.weight, std=0.01)\n",
        "        nn.init.normal_(self.frame_position_embeddings.weight, std=0.01)\n",
        "\n",
        "    def build_attention_mask(self, attn_window):\n",
        "        # lazily create causal attention mask, with full attention between the vision tokens\n",
        "        # pytorch uses additive attention mask; fill with -inf\n",
        "        mask = torch.empty(self.visual_length, self.visual_length)\n",
        "        mask.fill_(float('-inf'))\n",
        "        for i in range(int(self.visual_length / attn_window)):\n",
        "            if (i + 1) * attn_window < self.visual_length:\n",
        "                mask[i * attn_window: (i + 1) * attn_window, i * attn_window: (i + 1) * attn_window] = 0\n",
        "            else:\n",
        "                mask[i * attn_window: self.visual_length, i * attn_window: self.visual_length] = 0\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def adj4(self, x, seq_len):\n",
        "        soft = nn.Softmax(1)\n",
        "        x2 = x.matmul(x.permute(0, 2, 1)) # B*T*T\n",
        "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = x_norm.matmul(x_norm.permute(0, 2, 1))\n",
        "        x2 = x2/(x_norm_x+1e-20)\n",
        "        output = torch.zeros_like(x2)\n",
        "        if seq_len is None:\n",
        "            for i in range(x.shape[0]):\n",
        "                tmp = x2[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n",
        "    def encode_video(self, images, padding_mask, lengths):\n",
        "        images = images.to(torch.float)\n",
        "        position_ids = torch.arange(self.visual_length, device=self.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(images.shape[0], -1)\n",
        "        frame_position_embeddings = self.frame_position_embeddings(position_ids)\n",
        "        frame_position_embeddings = frame_position_embeddings.permute(1, 0, 2)\n",
        "        images = images.permute(1, 0, 2) + frame_position_embeddings\n",
        "\n",
        "        x, _ = self.temporal((images, None))\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        adj = self.adj4(x, lengths)\n",
        "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
        "        x1_h = self.gelu(self.gc1(x, adj))\n",
        "        x2_h = self.gelu(self.gc3(x, disadj))\n",
        "\n",
        "        x1 = self.gelu(self.gc2(x1_h, adj))\n",
        "        x2 = self.gelu(self.gc4(x2_h, disadj))\n",
        "\n",
        "        x = torch.cat((x1, x2), 2)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def encode_textprompt(self, text):\n",
        "        word_tokens = clip.tokenize(text).to(self.device)\n",
        "        word_embedding = self.clipmodel.encode_token(word_tokens)\n",
        "        text_embeddings = self.text_prompt_embeddings(torch.arange(77).to(self.device)).unsqueeze(0).repeat([len(text), 1, 1])\n",
        "        text_tokens = torch.zeros(len(text), 77).to(self.device)\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            ind = torch.argmax(word_tokens[i], -1)\n",
        "            text_embeddings[i, 0] = word_embedding[i, 0]\n",
        "            text_embeddings[i, self.prompt_prefix + 1: self.prompt_prefix + ind] = word_embedding[i, 1: ind]\n",
        "            text_embeddings[i, self.prompt_prefix + ind + self.prompt_postfix] = word_embedding[i, ind]\n",
        "            text_tokens[i, self.prompt_prefix + ind + self.prompt_postfix] = word_tokens[i, ind]\n",
        "\n",
        "        text_features = self.clipmodel.encode_text(text_embeddings, text_tokens)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(self, visual, padding_mask, text, lengths):\n",
        "        visual_features = self.encode_video(visual, padding_mask, lengths)\n",
        "        logits1 = self.classifier(visual_features + self.mlp2(visual_features))\n",
        "\n",
        "        text_features_ori = self.encode_textprompt(text)\n",
        "\n",
        "        text_features = text_features_ori\n",
        "        logits_attn = logits1.permute(0, 2, 1)\n",
        "        visual_attn = logits_attn @ visual_features\n",
        "        visual_attn = visual_attn / visual_attn.norm(dim=-1, keepdim=True)\n",
        "        visual_attn = visual_attn.expand(visual_attn.shape[0], text_features_ori.shape[0], visual_attn.shape[2])\n",
        "        text_features = text_features_ori.unsqueeze(0)\n",
        "        text_features = text_features.expand(visual_attn.shape[0], text_features.shape[1], text_features.shape[2])\n",
        "        text_features = text_features + visual_attn\n",
        "        text_features = text_features + self.mlp1(text_features)\n",
        "\n",
        "        visual_features_norm = visual_features / visual_features.norm(dim=-1, keepdim=True)\n",
        "        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        text_features_norm = text_features_norm.permute(0, 2, 1)\n",
        "        logits2 = visual_features_norm @ text_features_norm.type(visual_features_norm.dtype) / 0.07\n",
        "\n",
        "        return text_features_ori, logits1, logits2, visual_features\n",
        ""
      ],
      "metadata": {
        "id": "2ODZ8AzzN3_M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Function\n",
        "def test(model, testdataloader, maxlen, prompt_text, gt, gtsegments, gtlabels, device):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    element_logits2_stack = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, item in enumerate(testdataloader):\n",
        "            visual = item[0].squeeze(0)\n",
        "            length = item[2]\n",
        "\n",
        "            length = int(length)\n",
        "            len_cur = length\n",
        "            if len_cur < maxlen:\n",
        "                visual = visual.unsqueeze(0)\n",
        "\n",
        "            visual = visual.to(device)\n",
        "\n",
        "            lengths = torch.zeros(int(length / maxlen) + 1)\n",
        "            for j in range(int(length / maxlen) + 1):\n",
        "                if j == 0 and length < maxlen:\n",
        "                    lengths[j] = length\n",
        "                elif j == 0 and length > maxlen:\n",
        "                    lengths[j] = maxlen\n",
        "                    length -= maxlen\n",
        "                elif length > maxlen:\n",
        "                    lengths[j] = maxlen\n",
        "                    length -= maxlen\n",
        "                else:\n",
        "                    lengths[j] = length\n",
        "            lengths = lengths.to(int)\n",
        "            padding_mask = get_batch_mask(lengths, maxlen).to(device)\n",
        "            _, logits1, logits2, _ = model(visual, padding_mask, prompt_text, lengths)\n",
        "            logits1 = logits1.reshape(logits1.shape[0] * logits1.shape[1], logits1.shape[2])\n",
        "            logits2 = logits2.reshape(logits2.shape[0] * logits2.shape[1], logits2.shape[2])\n",
        "            prob2 = (1 - logits2[0:len_cur].softmax(dim=-1)[:, 0].squeeze(-1))\n",
        "            prob1 = torch.sigmoid(logits1[0:len_cur].squeeze(-1))\n",
        "\n",
        "            if i == 0:\n",
        "                ap1 = prob1\n",
        "                ap2 = prob2\n",
        "                #ap3 = prob3\n",
        "            else:\n",
        "                ap1 = torch.cat([ap1, prob1], dim=0)\n",
        "                ap2 = torch.cat([ap2, prob2], dim=0)\n",
        "\n",
        "            element_logits2 = logits2[0:len_cur].softmax(dim=-1).detach().cpu().numpy()\n",
        "            element_logits2 = np.repeat(element_logits2, 16, 0)\n",
        "            element_logits2_stack.append(element_logits2)\n",
        "\n",
        "    ap1 = ap1.cpu().numpy()\n",
        "    ap2 = ap2.cpu().numpy()\n",
        "    ap1 = ap1.tolist()\n",
        "    ap2 = ap2.tolist()\n",
        "\n",
        "    ROC1 = roc_auc_score(gt, np.repeat(ap1, 16))\n",
        "    AP1 = average_precision_score(gt, np.repeat(ap1, 16))\n",
        "    ROC2 = roc_auc_score(gt, np.repeat(ap2, 16))\n",
        "    AP2 = average_precision_score(gt, np.repeat(ap2, 16))\n",
        "\n",
        "    print(\"AUC1: \", ROC1, \" AP1: \", AP1)\n",
        "    print(\"AUC2: \", ROC2, \" AP2:\", AP2)\n",
        "\n",
        "    dmap, iou = dmAP(element_logits2_stack, gtsegments, gtlabels, excludeNormal=False)\n",
        "    averageMAP = 0\n",
        "    for i in range(5):\n",
        "        print('mAP@{0:.1f} ={1:.2f}%'.format(iou[i], dmap[i]))\n",
        "        averageMAP += dmap[i]\n",
        "    averageMAP = averageMAP/(i+1)\n",
        "    print('average MAP: {:.2f}'.format(averageMAP))\n",
        "\n",
        "    return ROC1, AP1, averageMAP\n",
        "\n",
        "\n",
        "def run_test():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    args = Args()\n",
        "\n",
        "    label_map = dict({'Normal': 'Normal', 'Abuse': 'Abuse', 'Arrest': 'Arrest', 'Arson': 'Arson', 'Assault': 'Assault', 'Burglary': 'Burglary', 'Explosion': 'Explosion', 'Fighting': 'Fighting', 'RoadAccidents': 'RoadAccidents', 'Robbery': 'Robbery', 'Shooting': 'Shooting', 'Shoplifting': 'Shoplifting', 'Stealing': 'Stealing', 'Vandalism': 'Vandalism'})\n",
        "\n",
        "    testdataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n",
        "    testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    prompt_text = get_prompt_text(label_map)\n",
        "    gt = np.load(args.gt_path)\n",
        "    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n",
        "    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n",
        "\n",
        "    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n",
        "    model_param = torch.load(args.model_path)\n",
        "    model.load_state_dict(model_param)\n",
        "\n",
        "    test(model, testdataloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)"
      ],
      "metadata": {
        "id": "8LrMWNfSZ-RX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Function\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TripletLoss, self).__init__()\n",
        "\n",
        "    def distance(self, x, y):\n",
        "        return torch.cdist(x, y, p=2)\n",
        "\n",
        "    def forward(self, feats, margin=100.0):\n",
        "        bs = feats.size(0)\n",
        "        n_feats = feats[:bs // 2]\n",
        "        a_feats = feats[bs // 2:]\n",
        "\n",
        "        # Shape: (N, N) and (N, A)\n",
        "        n_d = self.distance(n_feats, n_feats)\n",
        "        a_d = self.distance(n_feats, a_feats)\n",
        "\n",
        "        n_d_max, _ = torch.max(n_d, dim=0)         # shape: (N,)\n",
        "        a_d_min, _ = torch.min(a_d, dim=0)         # shape: (A,)\n",
        "\n",
        "        a_d_min = margin - a_d_min\n",
        "        a_d_min = torch.max(torch.zeros_like(a_d_min), a_d_min)  # element-wise clamp to >= 0\n",
        "\n",
        "        return torch.mean(n_d_max) + torch.mean(a_d_min)\n",
        "\n",
        "\n",
        "\n",
        "def CLASM(logits, labels, lengths, device):\n",
        "    instance_logits = torch.zeros(0).to(device)\n",
        "    labels = labels / torch.sum(labels, dim=1, keepdim=True)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True, dim=0)\n",
        "        instance_logits = torch.cat([instance_logits, torch.mean(tmp, 0, keepdim=True)], dim=0)\n",
        "\n",
        "    milloss = -torch.mean(torch.sum(labels * F.log_softmax(instance_logits, dim=1), dim=1), dim=0)\n",
        "    return milloss\n",
        "\n",
        "def CLAS2(logits, labels, lengths, device):\n",
        "    instance_logits = torch.zeros(0).to(device)\n",
        "    labels = 1 - labels[:, 0].reshape(labels.shape[0])\n",
        "    labels = labels.to(device)\n",
        "    logits = torch.sigmoid(logits).reshape(logits.shape[0], logits.shape[1])\n",
        "\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp, _ = torch.topk(logits[i, 0:lengths[i]], k=int(lengths[i] / 16 + 1), largest=True)\n",
        "        tmp = torch.mean(tmp).view(1)\n",
        "        instance_logits = torch.cat([instance_logits, tmp], dim=0)\n",
        "\n",
        "    clsloss = F.binary_cross_entropy(instance_logits, labels)\n",
        "    return clsloss\n",
        "\n",
        "\n",
        "def train(model, normal_loader, anomaly_loader, testloader, args, label_map, device, experiment):\n",
        "    model.to(device)\n",
        "    gt = np.load(args.gt_path)\n",
        "    gtsegments = np.load(args.gt_segment_path, allow_pickle=True)\n",
        "    gtlabels = np.load(args.gt_label_path, allow_pickle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    scheduler = MultiStepLR(optimizer, args.scheduler_milestones, args.scheduler_rate)\n",
        "    triplet_loss_fn = TripletLoss().to(device)\n",
        "    prompt_text = get_prompt_text(label_map)\n",
        "    auc_best = 0\n",
        "    epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if args.use_checkpoint and Path(args.checkpoint_path).exists():\n",
        "        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        auc_best = checkpoint['auc']\n",
        "        print(\"checkpoint info:\")\n",
        "        print(\"epoch:\", epoch+1, \" auc:\", auc_best)\n",
        "\n",
        "    for e in range(args.max_epoch):\n",
        "        curr_epoch = e + 1\n",
        "        experiment.log_current_epoch(curr_epoch)\n",
        "        model.train()\n",
        "        loss_total1 = 0\n",
        "        loss_total2 = 0\n",
        "        normal_iter = iter(normal_loader)\n",
        "        anomaly_iter = iter(anomaly_loader)\n",
        "\n",
        "        for i in range(min(len(normal_loader), len(anomaly_loader))):\n",
        "            step = i * normal_loader.batch_size * 2\n",
        "            global_step += 1\n",
        "\n",
        "            normal_features, normal_label, normal_lengths = next(normal_iter)\n",
        "            anomaly_features, anomaly_label, anomaly_lengths = next(anomaly_iter)\n",
        "\n",
        "            visual_features = torch.cat([normal_features, anomaly_features], dim=0).to(device)\n",
        "            text_labels = list(normal_label) + list(anomaly_label)\n",
        "            feat_lengths = torch.cat([normal_lengths, anomaly_lengths], dim=0).to(device)\n",
        "            text_labels = get_batch_label(text_labels, prompt_text, label_map).to(device)\n",
        "\n",
        "            text_features, logits1, logits2, visual_feats = model(visual_features, None, prompt_text, feat_lengths)\n",
        "\n",
        "            loss1 = CLAS2(logits1, text_labels, feat_lengths, device)\n",
        "            loss_total1 += loss1.item()\n",
        "\n",
        "            loss2 = CLASM(logits2, text_labels, feat_lengths, device)\n",
        "            loss_total2 += loss2.item()\n",
        "\n",
        "            triplet_loss_val = triplet_loss_fn(visual_feats)\n",
        "\n",
        "            loss3 = torch.zeros(1).to(device)\n",
        "            text_feature_normal = text_features[0] / text_features[0].norm(dim=-1, keepdim=True)\n",
        "            for j in range(1, text_features.shape[0]):\n",
        "                text_feature_abr = text_features[j] / text_features[j].norm(dim=-1, keepdim=True)\n",
        "                loss3 += torch.abs(text_feature_normal @ text_feature_abr)\n",
        "            loss3 = loss3 / 13 * 1e-1\n",
        "\n",
        "            loss = loss1 + loss2 + loss3 + 0.01 * triplet_loss_val\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss1 = loss_total1 / (i + 1)\n",
        "            avg_loss2 = loss_total2 / (i + 1)\n",
        "            loss3_val = loss3.item()\n",
        "\n",
        "            # Comet logging (per step)\n",
        "            experiment.log_metric(\"loss1\", avg_loss1, step=global_step)\n",
        "            experiment.log_metric(\"loss2\", avg_loss2, step=global_step)\n",
        "            experiment.log_metric(\"loss3\", loss3_val, step=global_step)\n",
        "            experiment.log_metric(\"loss_triplet\", triplet_loss_val.item(), step=global_step)\n",
        "\n",
        "\n",
        "            if step % 1280 == 0 and step != 0:\n",
        "                print(f'epoch: {e+1} | step: {step} | loss1: {avg_loss1:.4f} | loss2: {avg_loss2:.4f} | loss3: {loss3_val:.4f}')\n",
        "\n",
        "                # Evaluate\n",
        "                AUC, AP, averageMAP = test(model, testloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)\n",
        "                experiment.log_metric(\"AUC\", AUC, step=global_step)\n",
        "                experiment.log_metric(\"Average_MAP\", averageMAP, step=global_step)\n",
        "\n",
        "                if AUC > auc_best:\n",
        "                    auc_best = AUC\n",
        "                    experiment.log_metric(\"Best_AUC\", auc_best, step=global_step)\n",
        "\n",
        "                    checkpoint = {\n",
        "                        'epoch': e,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'auc': auc_best\n",
        "                    }\n",
        "                    torch.save(checkpoint, args.checkpoint_path)\n",
        "\n",
        "                    # Log checkpoint to Comet\n",
        "                    experiment.log_asset(file_data=args.checkpoint_path, file_name=\"best_checkpoint.pth\", overwrite=True)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        experiment.log_metric(\"epoch\", curr_epoch, step=global_step)\n",
        "\n",
        "        # Save current model weights separately\n",
        "        checkpoint_dir = os.path.dirname(args.checkpoint_path)\n",
        "        save_path = os.path.join(checkpoint_dir, 'model_cur.pth')\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        # Reload best checkpoint before next epoch\n",
        "        checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Save and log final model weights\n",
        "    checkpoint = torch.load(args.checkpoint_path, weights_only=False)\n",
        "    torch.save(checkpoint['model_state_dict'], args.model_path)\n",
        "    experiment.log_model(\"final_model\", args.model_path)\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    #torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def run_train(experiment):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    args = Args()\n",
        "    setup_seed(args.seed)\n",
        "\n",
        "    label_map = dict({'Normal': 'normal', 'Abuse': 'abuse', 'Arrest': 'arrest', 'Arson': 'arson', 'Assault': 'assault', 'Burglary': 'burglary', 'Explosion': 'explosion', 'Fighting': 'fighting', 'RoadAccidents': 'roadAccidents', 'Robbery': 'robbery', 'Shooting': 'shooting', 'Shoplifting': 'shoplifting', 'Stealing': 'stealing', 'Vandalism': 'vandalism'})\n",
        "\n",
        "    normal_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, True)\n",
        "    normal_loader = DataLoader(normal_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "    anomaly_dataset = UCFDataset(args.visual_length, args.train_list, False, label_map, False)\n",
        "    anomaly_loader = DataLoader(anomaly_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    test_dataset = UCFDataset(args.visual_length, args.test_list, True, label_map)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = CLIPVAD(args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head, args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device)\n",
        "\n",
        "    train(model, normal_loader, anomaly_loader, test_loader, args, label_map, device, experiment)\n",
        "\n"
      ],
      "metadata": {
        "id": "eyaLSBFMpdWk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMET_API_KEY = userdata.get('COMET_API_KEY')\n",
        "experiment = comet_ml.Experiment(\n",
        "                  api_key=COMET_API_KEY,\n",
        "                  project_name=\"vad_clip_notebook\")\n",
        "\n",
        "with experiment.train():\n",
        "  run_train(experiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBtt-FAfgY2A",
        "outputId": "0b6ddbc8-d89a-4957-9800-22d57a497108"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : industrial_meerkat_7303\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/hammad-ali/vad-clip-notebook/d6339932d1924cd793cb287aa97cfb47\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     curr_epoch             : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss             : 5.90816593170166\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss1 [4]        : (1.026927724480629, 1.573239803314209)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss2 [4]        : (2.600329875946045, 2.636782646179199)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss3 [4]        : (0.09771856665611267, 0.09786854684352875)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss_triplet [4] : (143.97821044921875, 160.02749633789062)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=https%3A%2F%2Fgithub.com%2Fhammad-ali1%2Fvad_clip_colab_notebook%2Fblob%2Fmain%2Fvad_clip_notebook.ipynb\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/hammad-ali/vad-clip-notebook/f9dba8f5dae342feb922c0419b36310d\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 | step: 1280 | loss1: 0.6216 | loss2: 2.5255 | loss3: 0.0969\n",
            "AUC1:  0.7665694552381426  AP1:  0.20504387473224167\n",
            "AUC2:  0.44358010276992765  AP2: 0.06440863824593616\n",
            "mAP@0.1 =1.55%\n",
            "mAP@0.2 =1.24%\n",
            "mAP@0.3 =0.92%\n",
            "mAP@0.4 =0.80%\n",
            "mAP@0.5 =0.49%\n",
            "average MAP: 1.00\n",
            "epoch: 1 | step: 2560 | loss1: 0.4372 | loss2: 2.4046 | loss3: 0.0949\n",
            "AUC1:  0.7958701043358748  AP1:  0.2212085710622907\n",
            "AUC2:  0.6642279432850482  AP2: 0.1291106817161897\n",
            "mAP@0.1 =2.69%\n",
            "mAP@0.2 =2.29%\n",
            "mAP@0.3 =2.10%\n",
            "mAP@0.4 =1.92%\n",
            "mAP@0.5 =1.64%\n",
            "average MAP: 2.13\n",
            "epoch: 2 | step: 1280 | loss1: 0.1598 | loss2: 1.9778 | loss3: 0.0863\n",
            "AUC1:  0.8068843817628866  AP1:  0.22744124632249016\n",
            "AUC2:  0.7750349382753521  AP2: 0.18589686376821643\n",
            "mAP@0.1 =5.42%\n",
            "mAP@0.2 =4.26%\n",
            "mAP@0.3 =3.47%\n",
            "mAP@0.4 =2.98%\n",
            "mAP@0.5 =2.23%\n",
            "average MAP: 3.67\n",
            "epoch: 2 | step: 2560 | loss1: 0.1487 | loss2: 1.8467 | loss3: 0.0607\n",
            "AUC1:  0.8202895275374185  AP1:  0.24191124147614412\n",
            "AUC2:  0.7933123445989194  AP2: 0.19331761910414835\n",
            "mAP@0.1 =5.46%\n",
            "mAP@0.2 =4.27%\n",
            "mAP@0.3 =2.73%\n",
            "mAP@0.4 =1.75%\n",
            "mAP@0.5 =0.63%\n",
            "average MAP: 2.97\n",
            "epoch: 3 | step: 1280 | loss1: 0.0756 | loss2: 1.4633 | loss3: 0.0417\n",
            "AUC1:  0.8374816765585807  AP1:  0.2613417100710356\n",
            "AUC2:  0.8153948049673069  AP2: 0.21106974361956188\n",
            "mAP@0.1 =7.30%\n",
            "mAP@0.2 =5.93%\n",
            "mAP@0.3 =4.20%\n",
            "mAP@0.4 =3.28%\n",
            "mAP@0.5 =2.69%\n",
            "average MAP: 4.68\n",
            "epoch: 3 | step: 2560 | loss1: 0.0690 | loss2: 1.4298 | loss3: 0.0290\n",
            "AUC1:  0.8415737127169372  AP1:  0.2636731851826313\n",
            "AUC2:  0.8236948285944097  AP2: 0.2163700215449455\n",
            "mAP@0.1 =8.34%\n",
            "mAP@0.2 =7.59%\n",
            "mAP@0.3 =5.02%\n",
            "mAP@0.4 =3.59%\n",
            "mAP@0.5 =2.90%\n",
            "average MAP: 5.49\n",
            "epoch: 4 | step: 1280 | loss1: 0.0413 | loss2: 1.3232 | loss3: 0.0212\n",
            "AUC1:  0.8481189026705707  AP1:  0.27069854103935614\n",
            "AUC2:  0.8332847586644425  AP2: 0.22484584962395554\n",
            "mAP@0.1 =10.26%\n",
            "mAP@0.2 =9.11%\n",
            "mAP@0.3 =6.46%\n",
            "mAP@0.4 =4.77%\n",
            "mAP@0.5 =3.27%\n",
            "average MAP: 6.77\n",
            "epoch: 4 | step: 2560 | loss1: 0.0383 | loss2: 1.3056 | loss3: 0.0233\n",
            "AUC1:  0.8416977877205671  AP1:  0.26343127581816456\n",
            "AUC2:  0.8325146290237173  AP2: 0.22502138638305716\n",
            "mAP@0.1 =8.80%\n",
            "mAP@0.2 =7.80%\n",
            "mAP@0.3 =4.96%\n",
            "mAP@0.4 =3.22%\n",
            "mAP@0.5 =2.87%\n",
            "average MAP: 5.53\n",
            "epoch: 5 | step: 1280 | loss1: 0.0316 | loss2: 1.3025 | loss3: 0.0207\n",
            "AUC1:  0.8468083783016112  AP1:  0.268763112876372\n",
            "AUC2:  0.8328618658470461  AP2: 0.2249284632730207\n",
            "mAP@0.1 =10.28%\n",
            "mAP@0.2 =9.14%\n",
            "mAP@0.3 =6.25%\n",
            "mAP@0.4 =5.78%\n",
            "mAP@0.5 =4.19%\n",
            "average MAP: 7.13\n",
            "epoch: 5 | step: 2560 | loss1: 0.0308 | loss2: 1.2974 | loss3: 0.0211\n",
            "AUC1:  0.844533666349276  AP1:  0.26565836046821323\n",
            "AUC2:  0.8308562588787909  AP2: 0.22365905116477558\n",
            "mAP@0.1 =8.90%\n",
            "mAP@0.2 =7.52%\n",
            "mAP@0.3 =3.70%\n",
            "mAP@0.4 =3.25%\n",
            "mAP@0.5 =2.56%\n",
            "average MAP: 5.19\n",
            "epoch: 6 | step: 1280 | loss1: 0.0333 | loss2: 1.2995 | loss3: 0.0214\n",
            "AUC1:  0.8456210672650549  AP1:  0.2672255166377818\n",
            "AUC2:  0.8312653274240938  AP2: 0.22316732997681984\n",
            "mAP@0.1 =9.53%\n",
            "mAP@0.2 =8.29%\n",
            "mAP@0.3 =4.45%\n",
            "mAP@0.4 =3.95%\n",
            "mAP@0.5 =3.10%\n",
            "average MAP: 5.86\n",
            "epoch: 6 | step: 2560 | loss1: 0.0330 | loss2: 1.2915 | loss3: 0.0223\n",
            "AUC1:  0.8445878190218632  AP1:  0.26591302498006136\n",
            "AUC2:  0.8308586097084327  AP2: 0.22282600741040057\n",
            "mAP@0.1 =8.69%\n",
            "mAP@0.2 =7.66%\n",
            "mAP@0.3 =3.84%\n",
            "mAP@0.4 =3.38%\n",
            "mAP@0.5 =2.64%\n",
            "average MAP: 5.24\n",
            "epoch: 7 | step: 1280 | loss1: 0.0304 | loss2: 1.2943 | loss3: 0.0216\n",
            "AUC1:  0.8454687018657738  AP1:  0.26679631861359665\n",
            "AUC2:  0.8310770233471274  AP2: 0.22279568523070678\n",
            "mAP@0.1 =9.46%\n",
            "mAP@0.2 =8.23%\n",
            "mAP@0.3 =4.44%\n",
            "mAP@0.4 =3.86%\n",
            "mAP@0.5 =3.11%\n",
            "average MAP: 5.82\n",
            "epoch: 7 | step: 2560 | loss1: 0.0323 | loss2: 1.2915 | loss3: 0.0227\n",
            "AUC1:  0.8437685674487693  AP1:  0.26428132307556307\n",
            "AUC2:  0.8296415029000788  AP2: 0.22150218614331427\n",
            "mAP@0.1 =9.16%\n",
            "mAP@0.2 =8.08%\n",
            "mAP@0.3 =4.29%\n",
            "mAP@0.4 =3.87%\n",
            "mAP@0.5 =3.12%\n",
            "average MAP: 5.70\n",
            "epoch: 8 | step: 1280 | loss1: 0.0317 | loss2: 1.2996 | loss3: 0.0213\n",
            "AUC1:  0.8458181920270119  AP1:  0.2671836967543598\n",
            "AUC2:  0.831540951194734  AP2: 0.22327556442233423\n",
            "mAP@0.1 =8.90%\n",
            "mAP@0.2 =7.63%\n",
            "mAP@0.3 =3.83%\n",
            "mAP@0.4 =3.20%\n",
            "mAP@0.5 =2.77%\n",
            "average MAP: 5.27\n",
            "epoch: 8 | step: 2560 | loss1: 0.0310 | loss2: 1.2916 | loss3: 0.0222\n",
            "AUC1:  0.8445489844134221  AP1:  0.26534752553588065\n",
            "AUC2:  0.8303923468782243  AP2: 0.22228931162195023\n",
            "mAP@0.1 =8.64%\n",
            "mAP@0.2 =7.59%\n",
            "mAP@0.3 =3.83%\n",
            "mAP@0.4 =3.27%\n",
            "mAP@0.5 =2.63%\n",
            "average MAP: 5.19\n",
            "epoch: 9 | step: 1280 | loss1: 0.0341 | loss2: 1.3054 | loss3: 0.0210\n",
            "AUC1:  0.8477569942681977  AP1:  0.27022658024727453\n",
            "AUC2:  0.8333100324694176  AP2: 0.22482285506275113\n",
            "mAP@0.1 =10.16%\n",
            "mAP@0.2 =9.03%\n",
            "mAP@0.3 =6.37%\n",
            "mAP@0.4 =4.75%\n",
            "mAP@0.5 =3.27%\n",
            "average MAP: 6.71\n",
            "epoch: 9 | step: 2560 | loss1: 0.0319 | loss2: 1.2985 | loss3: 0.0211\n",
            "AUC1:  0.8471825639898665  AP1:  0.26950426738539396\n",
            "AUC2:  0.8329248232649111  AP2: 0.2245280297790608\n",
            "mAP@0.1 =9.61%\n",
            "mAP@0.2 =8.50%\n",
            "mAP@0.3 =5.80%\n",
            "mAP@0.4 =4.23%\n",
            "mAP@0.5 =3.21%\n",
            "average MAP: 6.27\n",
            "epoch: 10 | step: 1280 | loss1: 0.0305 | loss2: 1.2975 | loss3: 0.0211\n",
            "AUC1:  0.8478961496303707  AP1:  0.2703425438852652\n",
            "AUC2:  0.8331536004045337  AP2: 0.22476803642771506\n",
            "mAP@0.1 =10.14%\n",
            "mAP@0.2 =9.01%\n",
            "mAP@0.3 =6.38%\n",
            "mAP@0.4 =4.75%\n",
            "mAP@0.5 =3.21%\n",
            "average MAP: 6.70\n",
            "epoch: 10 | step: 2560 | loss1: 0.0315 | loss2: 1.2996 | loss3: 0.0211\n",
            "AUC1:  0.8474519749793659  AP1:  0.26968714360504054\n",
            "AUC2:  0.8331299031483765  AP2: 0.22471975498900404\n",
            "mAP@0.1 =10.14%\n",
            "mAP@0.2 =9.00%\n",
            "mAP@0.3 =6.33%\n",
            "mAP@0.4 =4.71%\n",
            "mAP@0.5 =3.21%\n",
            "average MAP: 6.68\n"
          ]
        }
      ]
    }
  ]
}