{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hammad-ali1/vad_clip_colab_notebook/blob/main/vadclip_feature_extraction.ipynb)"
      ],
      "metadata": {
        "id": "ex7HgrQ9-UQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQXye8NHs8Ud",
        "outputId": "3d5b01da-27f5-46fd-ccf9-ccb43657a977"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXTRACTED_DATASET =  '/content/drive/MyDrive/Training-Normal-Videos-Part-1'\n",
        "SAVE_DIR = '/content/drive/MyDrive/UCFMambaClipFeatures'"
      ],
      "metadata": {
        "id": "rGDYvxjTibpd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "assert os.path.exists(SAVE_DIR)\n",
        "\n",
        "processed_files = len(glob.glob(os.path.join(SAVE_DIR, \"**\", \"*.npy\"), recursive=True)) // 10\n",
        "print(f'Already processed files: {processed_files}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51q-dwvasrbV",
        "outputId": "9c743a09-9b36-4441-bcd2-6ecb1273fc4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already processed files: 1213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert os.path.exists(EXTRACTED_DATASET)\n",
        "\n",
        "total_videos = len(glob.glob(os.path.join(EXTRACTED_DATASET, \"**\", \"*.mp4\"), recursive=True))\n",
        "print(f'Total Videos: {total_videos}')"
      ],
      "metadata": {
        "id": "zuhOKkYHAkKn",
        "outputId": "ae2c1af6-51df-4d3d-aed4-46c406947675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Videos: 430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ftfy\n",
        "!pip -q install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n",
        "!pip -q install causal-conv1d==1.4.0 && pip install mamba-ssm==2.2.2\n",
        "!pip -q install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrcS-6IkmGBu",
        "outputId": "a83aab6f-46c9-4ec4-a084-3be52805769b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pull Latest Repo\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "  gh_token = userdata.get('vad_clip_gh_token')\n",
        "except Exception:\n",
        "  gh_token = None\n",
        "\n",
        "REPO_NAME = \"vad_clip_colab_notebook\"\n",
        "REPO_PATH = Path(\"/content\") / REPO_NAME\n",
        "\n",
        "if gh_token:\n",
        "  GITHUB_URL = f\"https://{gh_token}@github.com/hammad-ali1/{REPO_NAME}.git\"\n",
        "else:\n",
        "  GITHUB_URL = f'https://github.com/hammad-ali1/{REPO_NAME}'\n",
        "\n",
        "!rm -rf {REPO_PATH}\n",
        "!git clone {GITHUB_URL}\n",
        "\n",
        "!cp -r {REPO_PATH}/* /content"
      ],
      "metadata": {
        "id": "0cJMfyytNQfU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/vmamba/kernels/selective_scan && pip install ."
      ],
      "metadata": {
        "id": "T3zisgXJi8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "\n",
        "import gc\n",
        "from collections import OrderedDict\n",
        "\n",
        "from models import CLIP_VMamba_S\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "jO009vqFOCPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def video_crop(video_frame, type):\n",
        "    l = video_frame.shape[0]\n",
        "    new_frame = []\n",
        "    for i in range(l):\n",
        "        img = cv2.resize(video_frame[i], dsize=(340, 256))\n",
        "        new_frame.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    #1\n",
        "    img = np.array(new_frame)\n",
        "    if type == 0:\n",
        "        img = img[:, 16:240, 58:282, :]\n",
        "    #2\n",
        "    elif type == 1:\n",
        "        img = img[:, :224, :224, :]\n",
        "    #3\n",
        "    elif type == 2:\n",
        "        img = img[:, :224, -224:, :]\n",
        "    #4\n",
        "    elif type == 3:\n",
        "        img = img[:, -224:, :224, :]\n",
        "    #5\n",
        "    elif type == 4:\n",
        "        img = img[:, -224:, -224:, :]\n",
        "    #6\n",
        "    elif type == 5:\n",
        "        img = img[:, 16:240, 58:282, :]\n",
        "        for i in range(img.shape[0]):\n",
        "            img[i] = cv2.flip(img[i], 1)\n",
        "    #7\n",
        "    elif type == 6:\n",
        "        img = img[:, :224, :224, :]\n",
        "        for i in range(img.shape[0]):\n",
        "            img[i] = cv2.flip(img[i], 1)\n",
        "    #8\n",
        "    elif type == 7:\n",
        "        img = img[:, :224, -224:, :]\n",
        "        for i in range(img.shape[0]):\n",
        "            img[i] = cv2.flip(img[i], 1)\n",
        "    #9\n",
        "    elif type == 8:\n",
        "        img = img[:, -224:, :224, :]\n",
        "        for i in range(img.shape[0]):\n",
        "            img[i] = cv2.flip(img[i], 1)\n",
        "    #10\n",
        "    elif type == 9:\n",
        "        img = img[:, -224:, -224:, :]\n",
        "        for i in range(img.shape[0]):\n",
        "            img[i] = cv2.flip(img[i], 1)\n",
        "\n",
        "    return img\n",
        "\n",
        "def image_crop(image, type):\n",
        "    img = cv2.resize(image, dsize=(340, 256))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    #1\n",
        "    if type == 0:\n",
        "        img = img[16:240, 58:282, :]\n",
        "    #2\n",
        "    elif type == 1:\n",
        "        img = img[:224, :224, :]\n",
        "    #3\n",
        "    elif type == 2:\n",
        "        img = img[:224, -224:, :]\n",
        "    #4\n",
        "    elif type == 3:\n",
        "        img = img[-224:, :224, :]\n",
        "    #5\n",
        "    elif type == 4:\n",
        "        img = img[-224:, -224:, :]\n",
        "    #6\n",
        "    elif type == 5:\n",
        "        img = img[16:240, 58:282, :]\n",
        "        img = cv2.flip(img, 1)\n",
        "    #7\n",
        "    elif type == 6:\n",
        "        img = img[:224, :224, :]\n",
        "        img = cv2.flip(img, 1)\n",
        "    #8\n",
        "    elif type == 7:\n",
        "        img = img[:224, -224:, :]\n",
        "        img = cv2.flip(img, 1)\n",
        "    #9\n",
        "    elif type == 8:\n",
        "        img = img[-224:, :224, :]\n",
        "        img = cv2.flip(img, 1)\n",
        "    #10\n",
        "    elif type == 9:\n",
        "        img = img[-224:, -224:, :]\n",
        "        img = cv2.flip(img, 1)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "Kc5DHlAUjBk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_frames_nonoverlap(video_path, stride=16, pick='center'):\n",
        "    \"\"\"\n",
        "    Return one frame per non-overlapping `stride`-frame chunk.\n",
        "    pick: 'center' | 'first' | 'last'\n",
        "    - For stride=16, 'center' -> index 7 (0-based) so you'll get floor(T/16) frames.\n",
        "    \"\"\"\n",
        "    assert stride >= 1\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    sampled = []\n",
        "    chunk = []\n",
        "    center_idx = (stride // 2 - 1) if (stride % 2 == 0) else (stride // 2)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        chunk.append(frame)\n",
        "        if len(chunk) == stride:\n",
        "            if pick == 'center':\n",
        "                idx = center_idx\n",
        "            elif pick == 'first':\n",
        "                idx = 0\n",
        "            elif pick == 'last':\n",
        "                idx = stride - 1\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown pick='{pick}'\")\n",
        "            sampled.append(chunk[idx])\n",
        "            chunk.clear()\n",
        "\n",
        "    cap.release()\n",
        "    if len(sampled) == 0:\n",
        "        return np.empty((0,), dtype=object)\n",
        "    return np.array(sampled)  # shape: (floor(T/stride), H, W, C)\n",
        "\n",
        "\n",
        "def extract_clip_features(video_path, save_dir, model, preprocess, device,\n",
        "                          stride=16, pick='center', batch_size=128):\n",
        "    # --- prepare output paths ---\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    parent_dir = os.path.basename(os.path.dirname(video_path))\n",
        "    out_parent = os.path.join(save_dir, parent_dir)\n",
        "    os.makedirs(out_parent, exist_ok=True)\n",
        "\n",
        "    # --- skip if all crops exist ---\n",
        "    if all(os.path.exists(os.path.join(out_parent, f\"{video_name}__{c}.npy\")) for c in range(10)):\n",
        "        return\n",
        "\n",
        "    # --- sample frames exactly floor(T/stride) ---\n",
        "    frames = _sample_frames_nonoverlap(video_path, stride=stride, pick=pick)\n",
        "    if frames.size == 0:\n",
        "        raise ValueError(f\"No frames found (after sampling) in {video_path}\")\n",
        "\n",
        "    for crop_type in tqdm(range(10),\n",
        "                          desc=f\"Video: {video_name}\",\n",
        "                          unit=\"crop\",\n",
        "                          position=1,\n",
        "                          leave=False,\n",
        "                          dynamic_ncols=True):\n",
        "        save_path = os.path.join(out_parent, f\"{video_name}__{crop_type}.npy\")\n",
        "        if os.path.exists(save_path):\n",
        "            continue\n",
        "\n",
        "        cropped_frames = video_crop(frames, crop_type)  # expects (N, H, W, C)\n",
        "\n",
        "        video_features = []\n",
        "        pbar = tqdm(total=len(cropped_frames),\n",
        "                    desc=f\"Crop {crop_type}\",\n",
        "                    unit=\"frame\",\n",
        "                    position=2,\n",
        "                    leave=False,\n",
        "                    dynamic_ncols=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(cropped_frames), batch_size):\n",
        "                batch_frames = cropped_frames[i:i+batch_size]\n",
        "\n",
        "                batch_tensors = [preprocess(Image.fromarray(f)) for f in batch_frames]\n",
        "                batch_tensors = torch.stack(batch_tensors).to(device)\n",
        "\n",
        "                # Encode\n",
        "                feats = model.encode_image(batch_tensors)\n",
        "                video_features.append(feats.cpu().numpy())\n",
        "\n",
        "                pbar.update(len(batch_frames))\n",
        "\n",
        "                # Free GPU memory\n",
        "                # del batch_tensors, feats\n",
        "                # torch.cuda.empty_cache()\n",
        "                # gc.collect()\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        video_features = np.vstack(video_features)  # (T, 512)\n",
        "        np.save(save_path, video_features)"
      ],
      "metadata": {
        "id": "7-zo1DFsnbDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_dataset_features(dataset_dir, save_dir, model, preprocess, device):\n",
        "    # find all mp4 files recursively\n",
        "    video_files = glob.glob(os.path.join(dataset_dir, \"**\", \"*.mp4\"), recursive=True)\n",
        "    video_files.sort()  # stable ordering\n",
        "\n",
        "    print(f\"Found {len(video_files)} videos in {dataset_dir}\")\n",
        "\n",
        "    for video_path in tqdm(video_files, desc=\"Dataset\", unit=\"video\", position=0):\n",
        "        try:\n",
        "            extract_clip_features(\n",
        "                video_path=video_path,\n",
        "                save_dir=save_dir,\n",
        "                model=model,\n",
        "                preprocess=preprocess,\n",
        "                device=device\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error processing {video_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "cqgOZ39RtVQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clip_model():\n",
        "  !curl -L -o VMamba_S_clip.pt \"https://huggingface.co/weiquan/mamba-clip/resolve/main/VMamba_S_clip.pt?download=true\"\n",
        "\n",
        "  mamba_clip_model = CLIP_VMamba_S()\n",
        "\n",
        "  ckpt_path = \"VMamba_S_clip.pt\"\n",
        "\n",
        "  ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
        "  state_dict = OrderedDict()\n",
        "  for k, v in ckpt['state_dict'].items():\n",
        "      state_dict[k.replace('module.', '')] = v\n",
        "\n",
        "  mamba_clip_model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "  return mamba_clip_model"
      ],
      "metadata": {
        "id": "vK4fB0AINZgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC"
      ],
      "metadata": {
        "id": "gXAj3EvbQEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def _transform(n_px=224):\n",
        "    return Compose([\n",
        "        Resize(n_px, interpolation=BICUBIC),\n",
        "        CenterCrop(n_px),\n",
        "        _convert_image_to_rgb,\n",
        "        ToTensor(),\n",
        "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "    ])"
      ],
      "metadata": {
        "id": "rvSQOegtPlkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = load_clip_model()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "iX4hhHudprTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = _transform()"
      ],
      "metadata": {
        "id": "VF97P48dQSSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_clip_features('/content/drive/MyDrive/Anomaly-Videos-Part-1/Abuse/Abuse001_x264.mp4', 'UCFMambaClipFeatures', model, preprocess, device)"
      ],
      "metadata": {
        "id": "2cniQFUjRzia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_dataset_features(EXTRACTED_DATASET, SAVE_DIR, model, preprocess, device)"
      ],
      "metadata": {
        "id": "QAJ1xEhrMevs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}